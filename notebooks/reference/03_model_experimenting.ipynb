{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the model experimentation and finalization. It covers EDA, outlier treatment, transformation, training, model evaluation and comparison across models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import shutil\n",
    "\n",
    "# standard third party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# impute missing values\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import KNNImputer, IterativeImputer, SimpleImputer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', message=\"pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\", \n",
    "                        category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', message=\"pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\",\n",
    "                        category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard code-template imports\n",
    "from ta_lib.core.api import (\n",
    "    create_context, get_dataframe, get_feature_names_from_column_transformer, string_cleaning,\n",
    "    get_package_path, display_as_tabs, save_pipeline, load_pipeline, initialize_environment,\n",
    "    load_dataset, save_dataset, DEFAULT_ARTIFACTS_PATH, list_datasets\n",
    ")\n",
    "\n",
    "import ta_lib.eda.api as eda\n",
    "from xgboost import XGBRegressor\n",
    "from ta_lib.regression.api import SKLStatsmodelOLS\n",
    "from ta_lib.regression.api import RegressionComparison, RegressionReport\n",
    "import ta_lib.reports.api as reports\n",
    "from ta_lib.data_processing.api import Outlier\n",
    "\n",
    "initialize_environment(debug=False, hide_warnings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts_folder = DEFAULT_ARTIFACTS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = op.join('conf', 'config.yml')\n",
    "context = create_context(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/raw/orders',\n",
       " '/raw/product',\n",
       " '/cleaned/orders',\n",
       " '/cleaned/product',\n",
       " '/cleaned/sales',\n",
       " '/processed/google_search',\n",
       " '/processed/sales',\n",
       " '/processed/social_media',\n",
       " '/processed/ground_truth',\n",
       " '/train/sales/features',\n",
       " '/train/sales/target',\n",
       " '/test/sales/features',\n",
       " '/test/sales/target',\n",
       " '/score/sales/output']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_datasets(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_search_data = load_dataset(context,  '/processed/google_search')\n",
    "sales_data = load_dataset(context, '/processed/sales')\n",
    "social_media_data = load_dataset(context, '/processed/social_media')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 EDA/Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Modularize the whole code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_ground_truth = sales_data.copy()\n",
    "social_media_ground_truth = social_media_data.copy()\n",
    "google_search_data_ground_truth = google_search_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_out_common_themes(dataframe: pd.DataFrame) -> tuple:\n",
    "    \"\"\"\n",
    "    Filters out the common themes.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    dataframe : pd.DataFrame\n",
    "        The dataframe to filter out the themes from.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple of dataframes.\n",
    "    \"\"\"\n",
    "    common_theme_list = set(sales_ground_truth['theme_name']) & set(social_media_ground_truth['theme_name']) & set(google_search_data_ground_truth['theme_name'])\n",
    "    \n",
    "    temp_dataframe = dataframe[dataframe['theme_name'].isin(common_theme_list)]\n",
    "    \n",
    "    return temp_dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_ground_truth = filter_out_common_themes(sales_ground_truth)\n",
    "social_media_ground_truth = filter_out_common_themes(social_media_ground_truth)\n",
    "google_search_data_ground_truth = filter_out_common_themes(google_search_data_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_ground_truth['theme_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_3_themes_by_sales_in_lbs(dataframe: pd.DataFrame) -> list:\n",
    "    \"\"\"\n",
    "    Fetches the top 3 sales by lbs.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    dataframe\n",
    "        The dataframe to use for getting the top 3 sales.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of the top 3 themes.\n",
    "    \"\"\"\n",
    "    \n",
    "    temp_dataframe = dataframe.copy()\n",
    "\n",
    "    total_sales_data = temp_dataframe[['date', 'theme_name', 'sales_lbs_value']]\n",
    "\n",
    "    total_sales_data = total_sales_data[total_sales_data['theme_name'] != 'No Claim']\n",
    "    total_sales_data['total_sales_by_theme'] = total_sales_data.groupby(['theme_name'])['sales_lbs_value'].transform('sum')\n",
    "\n",
    "    total_sales_data = total_sales_data[['theme_name', 'total_sales_by_theme']]\n",
    "    total_sales_data.drop_duplicates(inplace=True)\n",
    "\n",
    "    # We are taking the top 3 themes by sales in lbs\n",
    "    total_sales_data.sort_values(by=['total_sales_by_theme'], ascending=False, inplace=True)\n",
    "\n",
    "    total_sales_data = total_sales_data[:3]\n",
    "\n",
    "    top_themes = total_sales_data['theme_name'].unique().tolist()\n",
    "    \n",
    "    return top_themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_themes = get_top_3_themes_by_sales_in_lbs(sales_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['low carb', 'no additives/preservatives', 'salmon']\n"
     ]
    }
   ],
   "source": [
    "print(top_themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_out_top_three_themes(dataframe_list : pd.DataFrame, theme_list: list) -> None:\n",
    "    \"\"\"\n",
    "    Filter out the top 3 themes from the dataframes provided.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    dataframe_list : list[pd.DataFrame]\n",
    "        List of dataframes to apply the filtering on.\n",
    "        \n",
    "    theme_list : list\n",
    "        The list of themes to use for filtering.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    for dataframe in dataframe_list:\n",
    "        dataframe = dataframe[dataframe['theme_name'].isin(theme_list)]\n",
    "    print('Dataframes filtered successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes filtered successfully!\n"
     ]
    }
   ],
   "source": [
    "filter_out_top_three_themes([sales_ground_truth, social_media_ground_truth, google_search_data_ground_truth], top_themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "social_media_ground_truth['theme_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking only the top 3 themes\n",
    "sales_ground_truth = sales_ground_truth[sales_ground_truth['theme_name'].isin(top_themes)]\n",
    "social_media_ground_truth = social_media_ground_truth[social_media_ground_truth['theme_name'].isin(top_themes)]\n",
    "google_search_data_ground_truth = google_search_data_ground_truth[google_search_data_ground_truth['theme_name'].isin(top_themes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(632, 9)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_search_data_ground_truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_google_search_data(google_search_ground_truth: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregates the google search data.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    google_search_ground_truth : pd.DataFrame\n",
    "        The google search dataframe to aggregate.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The aggregated dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    temp_dataframe = google_search_data.copy()\n",
    "    required_cols = ['date','theme_name'\n",
    "     , 'google_searchVolume', 'amazon_searchVolume', 'chewy_searchVolume', 'walmart_searchVolume', 'total_searchVolume']\n",
    "    temp_dataframe = temp_dataframe.filter(required_cols)\n",
    "\n",
    "\n",
    "    temp_dataframe.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return temp_dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_search_data_ground_truth = aggregate_google_search_data(google_search_data_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28600, 7)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_search_data_ground_truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_social_media_data(social_media_ground_truth: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregates the google search data.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    social_media_ground_truth : pd.DataFrame\n",
    "        The social media data to aggregate.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The aggregated dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    temp_dataframe = social_media_ground_truth.copy()\n",
    "    required_cols = ['date', 'theme_name', 'total_post']\n",
    "\n",
    "    temp_dataframe = temp_dataframe.filter(required_cols)\n",
    "\n",
    "\n",
    "    temp_dataframe.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return temp_dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_media_ground_truth = aggregate_social_media_data(social_media_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>theme_name</th>\n",
       "      <th>total_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>2018-12-15</td>\n",
       "      <td>low carb</td>\n",
       "      <td>2531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>2018-10-13</td>\n",
       "      <td>low carb</td>\n",
       "      <td>2096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2015-06-20</td>\n",
       "      <td>low carb</td>\n",
       "      <td>913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2015-10-17</td>\n",
       "      <td>low carb</td>\n",
       "      <td>1506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9555</th>\n",
       "      <td>2018-10-27</td>\n",
       "      <td>salmon</td>\n",
       "      <td>1935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9553</th>\n",
       "      <td>2018-10-13</td>\n",
       "      <td>salmon</td>\n",
       "      <td>603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1288</th>\n",
       "      <td>2016-03-26</td>\n",
       "      <td>no additives/preservatives</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9583</th>\n",
       "      <td>2019-05-11</td>\n",
       "      <td>salmon</td>\n",
       "      <td>981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>no additives/preservatives</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2015-09-19</td>\n",
       "      <td>low carb</td>\n",
       "      <td>1809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date                  theme_name  total_post\n",
       "196   2018-12-15                    low carb        2531\n",
       "187   2018-10-13                    low carb        2096\n",
       "14    2015-06-20                    low carb         913\n",
       "31    2015-10-17                    low carb        1506\n",
       "9555  2018-10-27                      salmon        1935\n",
       "9553  2018-10-13                      salmon         603\n",
       "1288  2016-03-26  no additives/preservatives          37\n",
       "9583  2019-05-11                      salmon         981\n",
       "1454  2019-06-01  no additives/preservatives         226\n",
       "27    2015-09-19                    low carb        1809"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "social_media_ground_truth.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_sales_data(sales_ground_truth: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate the sales data.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    sales_ground_truth : pd.DataFrame\n",
    "        The sales data to aggregate.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The aggregated sales data.\n",
    "    \"\"\"\n",
    "    temp_dataframe = sales_ground_truth.copy()\n",
    "    temp_dataframe['date'] = pd.to_datetime(temp_dataframe['date'])\n",
    "\n",
    "\n",
    "    required_cols = ['date', 'sales_dollars_value', 'sales_units_value', 'sales_lbs_value', 'vendor', 'theme_name']\n",
    "    temp_dataframe = temp_dataframe.filter(required_cols)\n",
    "\n",
    "    temp_dataframe['weekly_lbs_value'] = temp_dataframe.groupby(['date', 'theme_name', 'vendor'])['sales_lbs_value'].transform('sum')\n",
    "    temp_dataframe['weekly_units_value'] = temp_dataframe.groupby(['date', 'theme_name', 'vendor'])['sales_units_value'].transform('sum')\n",
    "    temp_dataframe['weekly_dollars_value'] = temp_dataframe.groupby(['date', 'theme_name', 'vendor'])['sales_dollars_value'].transform('sum')\n",
    "    temp_dataframe = temp_dataframe[['date', 'theme_name', 'vendor', 'weekly_lbs_value', 'weekly_units_value', 'weekly_dollars_value']]\n",
    "    temp_dataframe.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return temp_dataframe\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_ground_truth = aggregate_sales_data(sales_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date_columns_to_datetime():\n",
    "    sales_ground_truth['date'] = pd.to_datetime(sales_ground_truth['date'])\n",
    "    social_media_ground_truth['date'] = pd.to_datetime(social_media_ground_truth['date'])\n",
    "    google_search_data_ground_truth['date'] = pd.to_datetime(google_search_data_ground_truth['date'])\n",
    "    \n",
    "    return sales_ground_truth, social_media_ground_truth, google_search_data_ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_ground_truth, social_media_ground_truth, google_search_data_ground_truth = convert_date_columns_to_datetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_merged_data(sales_ground_truth: pd.DataFrame, social_media_ground_truth: pd.DataFrame, google_search_ground_truth : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates the final ground truth to use further downstream.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    sales_ground_truth : pd.DataFrame\n",
    "        The sales ground truth to use for the final merging process.\n",
    "    social_media_ground_truth : pd.DataFrame\n",
    "        The social ground truth data to use for the final merging process.\n",
    "    google_search_ground_truth : pd.DataFrame\n",
    "        The google search ground truth data to use for the final merging process.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The merged dataframe\n",
    "    \"\"\"\n",
    "    combined_ground_truth = pd.merge(sales_ground_truth, social_media_ground_truth, on=['date', 'theme_name'])\n",
    "    combined_ground_truth = pd.merge(combined_ground_truth, google_search_ground_truth, on=['date', 'theme_name'])\n",
    "    return combined_ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ground_truth = create_final_merged_data(sales_ground_truth, social_media_ground_truth, google_search_data_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2940, 12)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_ground_truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_vendor_wise_data(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process the vendor wise data.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    dataframe : pd.DataFrame\n",
    "        The dataframe containing the vendor wise data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The processed dataframe containing client and competitor related features.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Client related features\n",
    "    client_dataframe = dataframe[dataframe['vendor'] == 'A']\n",
    "    client_dataframe.rename(columns={'weekly_lbs_value': 'client_lbs_value'\n",
    "                                     , 'weekly_units_value': 'client_units_value'\n",
    "                                     , 'weekly_dollars_value': 'client_dollars_value'}, inplace=True)\n",
    "    \n",
    "    # Competitor related features\n",
    "    competitor_dataframe = dataframe[dataframe['vendor'] != 'A']\n",
    "    competitor_dataframe['competitor_lbs_value'] = competitor_dataframe.groupby(['date', 'theme_name'])['weekly_lbs_value'].transform('sum')\n",
    "    competitor_dataframe['competitor_units_value'] = competitor_dataframe.groupby(['date', 'theme_name'])['weekly_units_value'].transform('sum')\n",
    "    competitor_dataframe['competitor_dollars_value'] = competitor_dataframe.groupby(['date', 'theme_name'])['weekly_dollars_value'].transform('sum')\n",
    "    \n",
    "    client_dataframe = client_dataframe[['date', 'theme_name', 'client_lbs_value', 'client_units_value', 'client_dollars_value']]\n",
    "    competitor_dataframe = competitor_dataframe[['date', 'theme_name', 'competitor_lbs_value', 'competitor_units_value', 'competitor_dollars_value']]\n",
    "    \n",
    "    dataframe = pd.merge(dataframe, client_dataframe, on=['date', 'theme_name'])\n",
    "    dataframe = pd.merge(dataframe, competitor_dataframe, on=['date', 'theme_name'])\n",
    "    \n",
    "    dataframe = dataframe[['date', 'theme_name', 'total_post'\n",
    "                                                    , 'google_searchVolume', 'amazon_searchVolume'\n",
    "                                                    , 'chewy_searchVolume', 'walmart_searchVolume'\n",
    "                                                    , 'total_searchVolume', 'client_lbs_value',\n",
    "                                                    'client_units_value', 'client_dollars_value',\n",
    "                                                    'competitor_lbs_value', 'competitor_units_value',\n",
    "                                                    'competitor_dollars_value'\n",
    "                                                   ]]\n",
    "    dataframe.drop_duplicates(inplace=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ground_truth = process_vendor_wise_data(combined_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>theme_name</th>\n",
       "      <th>total_post</th>\n",
       "      <th>google_searchVolume</th>\n",
       "      <th>amazon_searchVolume</th>\n",
       "      <th>chewy_searchVolume</th>\n",
       "      <th>walmart_searchVolume</th>\n",
       "      <th>total_searchVolume</th>\n",
       "      <th>client_lbs_value</th>\n",
       "      <th>client_units_value</th>\n",
       "      <th>client_dollars_value</th>\n",
       "      <th>competitor_lbs_value</th>\n",
       "      <th>competitor_units_value</th>\n",
       "      <th>competitor_dollars_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-09</td>\n",
       "      <td>salmon</td>\n",
       "      <td>47</td>\n",
       "      <td>1913.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1913.0</td>\n",
       "      <td>5.379338e+06</td>\n",
       "      <td>754859</td>\n",
       "      <td>7.639690e+06</td>\n",
       "      <td>4.554449e+06</td>\n",
       "      <td>657551</td>\n",
       "      <td>4.899568e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2016-02-06</td>\n",
       "      <td>salmon</td>\n",
       "      <td>47</td>\n",
       "      <td>850.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>850.0</td>\n",
       "      <td>5.217845e+06</td>\n",
       "      <td>745221</td>\n",
       "      <td>7.433237e+06</td>\n",
       "      <td>5.106687e+06</td>\n",
       "      <td>645078</td>\n",
       "      <td>4.990354e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2016-02-20</td>\n",
       "      <td>salmon</td>\n",
       "      <td>156</td>\n",
       "      <td>1063.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1063.0</td>\n",
       "      <td>4.747996e+06</td>\n",
       "      <td>687313</td>\n",
       "      <td>6.826838e+06</td>\n",
       "      <td>4.962688e+06</td>\n",
       "      <td>669369</td>\n",
       "      <td>4.958131e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2016-02-27</td>\n",
       "      <td>salmon</td>\n",
       "      <td>90</td>\n",
       "      <td>1912.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1912.0</td>\n",
       "      <td>4.676554e+06</td>\n",
       "      <td>680633</td>\n",
       "      <td>6.724496e+06</td>\n",
       "      <td>4.954023e+06</td>\n",
       "      <td>667029</td>\n",
       "      <td>4.956457e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>2016-03-26</td>\n",
       "      <td>salmon</td>\n",
       "      <td>50</td>\n",
       "      <td>1912.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1912.0</td>\n",
       "      <td>4.739258e+06</td>\n",
       "      <td>705062</td>\n",
       "      <td>6.837560e+06</td>\n",
       "      <td>4.807323e+06</td>\n",
       "      <td>640089</td>\n",
       "      <td>4.817914e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17430</th>\n",
       "      <td>2019-09-07</td>\n",
       "      <td>low carb</td>\n",
       "      <td>1891</td>\n",
       "      <td>38008.0</td>\n",
       "      <td>5236.0</td>\n",
       "      <td>4489.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>48360.0</td>\n",
       "      <td>4.028791e+06</td>\n",
       "      <td>2695393</td>\n",
       "      <td>1.331764e+07</td>\n",
       "      <td>2.992734e+07</td>\n",
       "      <td>12652417</td>\n",
       "      <td>8.959849e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17472</th>\n",
       "      <td>2019-09-14</td>\n",
       "      <td>low carb</td>\n",
       "      <td>1142</td>\n",
       "      <td>41306.0</td>\n",
       "      <td>4655.0</td>\n",
       "      <td>4862.0</td>\n",
       "      <td>669.0</td>\n",
       "      <td>51492.0</td>\n",
       "      <td>4.017835e+06</td>\n",
       "      <td>2682848</td>\n",
       "      <td>1.327477e+07</td>\n",
       "      <td>2.982720e+07</td>\n",
       "      <td>12612445</td>\n",
       "      <td>8.869762e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17514</th>\n",
       "      <td>2019-09-21</td>\n",
       "      <td>low carb</td>\n",
       "      <td>1158</td>\n",
       "      <td>35338.0</td>\n",
       "      <td>3699.0</td>\n",
       "      <td>2952.0</td>\n",
       "      <td>625.0</td>\n",
       "      <td>42614.0</td>\n",
       "      <td>3.974981e+06</td>\n",
       "      <td>2641925</td>\n",
       "      <td>1.313288e+07</td>\n",
       "      <td>2.985235e+07</td>\n",
       "      <td>12537084</td>\n",
       "      <td>8.910157e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17556</th>\n",
       "      <td>2019-09-28</td>\n",
       "      <td>low carb</td>\n",
       "      <td>1545</td>\n",
       "      <td>33610.0</td>\n",
       "      <td>6315.0</td>\n",
       "      <td>2661.0</td>\n",
       "      <td>960.0</td>\n",
       "      <td>43546.0</td>\n",
       "      <td>3.980566e+06</td>\n",
       "      <td>2684050</td>\n",
       "      <td>1.301893e+07</td>\n",
       "      <td>2.997781e+07</td>\n",
       "      <td>12565509</td>\n",
       "      <td>8.939163e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17598</th>\n",
       "      <td>2019-10-05</td>\n",
       "      <td>low carb</td>\n",
       "      <td>1932</td>\n",
       "      <td>16560.0</td>\n",
       "      <td>2569.0</td>\n",
       "      <td>1701.0</td>\n",
       "      <td>458.0</td>\n",
       "      <td>21288.0</td>\n",
       "      <td>4.094692e+06</td>\n",
       "      <td>2725010</td>\n",
       "      <td>1.341184e+07</td>\n",
       "      <td>3.062778e+07</td>\n",
       "      <td>12827470</td>\n",
       "      <td>9.129245e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>420 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date theme_name  total_post  google_searchVolume  \\\n",
       "0     2016-01-09     salmon          47               1913.0   \n",
       "42    2016-02-06     salmon          47                850.0   \n",
       "84    2016-02-20     salmon         156               1063.0   \n",
       "126   2016-02-27     salmon          90               1912.0   \n",
       "168   2016-03-26     salmon          50               1912.0   \n",
       "...          ...        ...         ...                  ...   \n",
       "17430 2019-09-07   low carb        1891              38008.0   \n",
       "17472 2019-09-14   low carb        1142              41306.0   \n",
       "17514 2019-09-21   low carb        1158              35338.0   \n",
       "17556 2019-09-28   low carb        1545              33610.0   \n",
       "17598 2019-10-05   low carb        1932              16560.0   \n",
       "\n",
       "       amazon_searchVolume  chewy_searchVolume  walmart_searchVolume  \\\n",
       "0                      0.0                 0.0                   0.0   \n",
       "42                     0.0                 0.0                   0.0   \n",
       "84                     0.0                 0.0                   0.0   \n",
       "126                    0.0                 0.0                   0.0   \n",
       "168                    0.0                 0.0                   0.0   \n",
       "...                    ...                 ...                   ...   \n",
       "17430               5236.0              4489.0                 627.0   \n",
       "17472               4655.0              4862.0                 669.0   \n",
       "17514               3699.0              2952.0                 625.0   \n",
       "17556               6315.0              2661.0                 960.0   \n",
       "17598               2569.0              1701.0                 458.0   \n",
       "\n",
       "       total_searchVolume  client_lbs_value  client_units_value  \\\n",
       "0                  1913.0      5.379338e+06              754859   \n",
       "42                  850.0      5.217845e+06              745221   \n",
       "84                 1063.0      4.747996e+06              687313   \n",
       "126                1912.0      4.676554e+06              680633   \n",
       "168                1912.0      4.739258e+06              705062   \n",
       "...                   ...               ...                 ...   \n",
       "17430             48360.0      4.028791e+06             2695393   \n",
       "17472             51492.0      4.017835e+06             2682848   \n",
       "17514             42614.0      3.974981e+06             2641925   \n",
       "17556             43546.0      3.980566e+06             2684050   \n",
       "17598             21288.0      4.094692e+06             2725010   \n",
       "\n",
       "       client_dollars_value  competitor_lbs_value  competitor_units_value  \\\n",
       "0              7.639690e+06          4.554449e+06                  657551   \n",
       "42             7.433237e+06          5.106687e+06                  645078   \n",
       "84             6.826838e+06          4.962688e+06                  669369   \n",
       "126            6.724496e+06          4.954023e+06                  667029   \n",
       "168            6.837560e+06          4.807323e+06                  640089   \n",
       "...                     ...                   ...                     ...   \n",
       "17430          1.331764e+07          2.992734e+07                12652417   \n",
       "17472          1.327477e+07          2.982720e+07                12612445   \n",
       "17514          1.313288e+07          2.985235e+07                12537084   \n",
       "17556          1.301893e+07          2.997781e+07                12565509   \n",
       "17598          1.341184e+07          3.062778e+07                12827470   \n",
       "\n",
       "       competitor_dollars_value  \n",
       "0                  4.899568e+06  \n",
       "42                 4.990354e+06  \n",
       "84                 4.958131e+06  \n",
       "126                4.956457e+06  \n",
       "168                4.817914e+06  \n",
       "...                         ...  \n",
       "17430              8.959849e+07  \n",
       "17472              8.869762e+07  \n",
       "17514              8.910157e+07  \n",
       "17556              8.939163e+07  \n",
       "17598              9.129245e+07  \n",
       "\n",
       "[420 rows x 14 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_for_modelling = combined_ground_truth[['date', 'theme_name', 'total_post'\n",
    "                                                    , 'google_searchVolume', 'amazon_searchVolume'\n",
    "                                                    , 'chewy_searchVolume', 'walmart_searchVolume'\n",
    "                                                    , 'total_searchVolume', 'client_lbs_value',\n",
    "                                                    'client_units_value', 'client_dollars_value',\n",
    "                                                    'competitor_lbs_value', 'competitor_units_value',\n",
    "                                                    'competitor_dollars_value'\n",
    "                                                   ]]\n",
    "ground_truth_for_modelling.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset(context, ground_truth_for_modelling, '/processed/ground_truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X = ground_truth_for_modelling.drop(columns=['client_lbs_value'])\n",
    "target = y = ground_truth_for_modelling['client_lbs_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/raw/orders',\n",
       " '/raw/product',\n",
       " '/cleaned/orders',\n",
       " '/cleaned/product',\n",
       " '/cleaned/sales',\n",
       " '/processed/google_search',\n",
       " '/processed/sales',\n",
       " '/processed/social_media',\n",
       " '/processed/ground_truth',\n",
       " '/train/sales/features',\n",
       " '/train/sales/target',\n",
       " '/test/sales/features',\n",
       " '/test/sales/target',\n",
       " '/score/sales/output']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_datasets(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Move this to 2nd notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset(context, X_train, '/train/sales/features')\n",
    "save_dataset(context, y_train, '/train/sales/target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset(context, X_test, '/test/sales/features')\n",
    "save_dataset(context, y_test, '/test/sales/target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Feature Engineering\n",
    "\n",
    "The focus here is the `Pipeline` and not the model. Though the model would inform the pipeline that is needed to train the model, our focus is to set it up in such a way that it can be saved/loaded, tweaked for different model choices and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Read the Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check why the shape is having a mismatch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(294, 13) (294, 1)\n",
      "(126, 13) (126, 1)\n"
     ]
    }
   ],
   "source": [
    "train_X = load_dataset(context, 'train/sales/features')\n",
    "train_y = load_dataset(context, 'train/sales/target')\n",
    "print(train_X.shape, train_y.shape)\n",
    "\n",
    "test_X = load_dataset(context, 'test/sales/features')\n",
    "test_y = load_dataset(context, 'test/sales/target')\n",
    "print(test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Feature Engineering Pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dev NOTES**\n",
    "\n",
    "For Feature Engineering and Model Building sklearn.pipeline.Pipeline are leveraged because of the following advantages\n",
    "<details>\n",
    "    \n",
    "1. It helps in automating workflows and are easier to read and comprehend.\n",
    "2. Right Sequence can be ensured and (for example always encodes before imputing)\n",
    "3. Reproducibility is very convenient with pipelines\n",
    "4. Pipelines help you prevent data leakage in your test data\n",
    "5. Code is near implementation ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Steps in the Feature Transformation are as follows\n",
    " - Outlier Treatment\n",
    " - Encoding of Categorical Columns\n",
    " - Missing Values Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting different types of columns for transformations\n",
    "cat_columns = train_X.select_dtypes('object').columns\n",
    "num_columns = train_X.select_dtypes('number').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['theme_name'], dtype='object')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['total_post', 'google_searchVolume', 'amazon_searchVolume',\n",
       "       'chewy_searchVolume', 'walmart_searchVolume', 'total_searchVolume',\n",
       "       'client_units_value', 'client_dollars_value', 'competitor_lbs_value',\n",
       "       'competitor_units_value', 'competitor_dollars_value'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier Handling\n",
    "- A Custom Transformer is used to handle outliers. It is not included as part of the pipeline as outliers handling are optional for test data\n",
    "- An option to either drop or cap the outliers can be passed during the transform call\n",
    "- If we want to treat outliers for some columns them we can pass cols argument to the Transformer\n",
    "- This will go into production code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(294, 13)\n",
      "(294, 13)\n"
     ]
    }
   ],
   "source": [
    "outlier_transformer = Outlier(method='median')\n",
    "print(train_X.shape)\n",
    "train_X = outlier_transformer.fit_transform(train_X)\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some sample pipelines showcasing how to create column specific pipelines and integrating them overall is presented below\n",
    "\n",
    "- Commonly target encoding is done for categorical variables with too many levels.\n",
    "- We also group sparse levels. For fewer levels one hot encoding/label encoding is preferred.\n",
    "- If there is one dominant level, we can use binary encoding.\n",
    "- This will go into production code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Remove mode imputer\n",
    "cat_imputer = Pipeline([\n",
    "    ('simple_impute', SimpleImputer(strategy='most_frequent')),\n",
    "])\n",
    "\n",
    "\n",
    "# NOTE: the list of transformations here are not sequential but weighted \n",
    "# (if multiple transforms are specified for a particular column)\n",
    "# for sequential transforms use a pipeline as shown above.\n",
    "features_transformer = ColumnTransformer([\n",
    "    \n",
    "    ## categorical columns\n",
    "    ('one_hot_enc', OneHotEncoder(drop='first'),\n",
    "     list(set(cat_columns))),\n",
    "    \n",
    "    # NOTE: if the same column gets repeated, then they are weighed in the final output\n",
    "    # If we want a sequence of operations, then we use a pipeline but that doesen't YET support\n",
    "    # get_feature_names. \n",
    "    ('cat_variable_imputer', cat_imputer, ['theme_name']),\n",
    "        \n",
    "    ## numeric columns\n",
    "    ('med_enc', SimpleImputer(strategy='median'), num_columns),\n",
    "    \n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dev notes(Encoding):**\n",
    "<details>\n",
    "\n",
    "    Some common practices followed in Categorical Feature Encoding are\n",
    "    * For categorical variables with too many levels, target encoding can be done.\n",
    "    * For fewer levels, one hot encoding can be done.\n",
    "    * If one very dominant level is observed, binary encoding can be used.\n",
    "    \n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Feature analysis\n",
    "\n",
    "Using the pipeline above analyze the features and decide on additional features to add/remove from the pipeline. This section will not be part of the production code, unless input data drifts etc. are explicitly demanded in the project.\n",
    "\n",
    "Here we are primarily focused on feature selection/elimination based on business rules, prior knowledge, data analysis.\n",
    "\n",
    "**We are not building any models at this point.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we create some sample data to analyze that we assume represent the population\n",
    "- train the features transformer and do the analysis as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_X = train_X.sample(frac=0.1, random_state=context.random_seed)\n",
    "sample_y = train_y.loc[sample_X.index]\n",
    "\n",
    "sample_train_X = get_dataframe(\n",
    "    features_transformer.fit_transform(sample_X, sample_y), \n",
    "    get_feature_names_from_column_transformer(features_transformer)\n",
    ")\n",
    "\n",
    "# nothing to do for target\n",
    "sample_train_y = sample_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the features transformer on the complete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = get_dataframe(\n",
    "    features_transformer.fit_transform(train_X, train_y), \n",
    "    get_feature_names_from_column_transformer(features_transformer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check why the additional column of cat_variable_imputer_theme_name is being created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>theme_name_no additives/preservatives</th>\n",
       "      <th>theme_name_salmon</th>\n",
       "      <th>cat_variable_imputer_theme_name</th>\n",
       "      <th>total_post</th>\n",
       "      <th>google_searchVolume</th>\n",
       "      <th>amazon_searchVolume</th>\n",
       "      <th>chewy_searchVolume</th>\n",
       "      <th>walmart_searchVolume</th>\n",
       "      <th>total_searchVolume</th>\n",
       "      <th>client_units_value</th>\n",
       "      <th>client_dollars_value</th>\n",
       "      <th>competitor_lbs_value</th>\n",
       "      <th>competitor_units_value</th>\n",
       "      <th>competitor_dollars_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>salmon</td>\n",
       "      <td>113.0</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>728731.0</td>\n",
       "      <td>6867987.0</td>\n",
       "      <td>4499792.0</td>\n",
       "      <td>653720.0</td>\n",
       "      <td>4420861.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>salmon</td>\n",
       "      <td>2649.0</td>\n",
       "      <td>12052.0</td>\n",
       "      <td>532.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12584.0</td>\n",
       "      <td>719860.0</td>\n",
       "      <td>7872415.0</td>\n",
       "      <td>4917567.0</td>\n",
       "      <td>784836.0</td>\n",
       "      <td>4494954.899164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>low carb</td>\n",
       "      <td>1781.0</td>\n",
       "      <td>39684.0</td>\n",
       "      <td>1345.0</td>\n",
       "      <td>101.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49955.0</td>\n",
       "      <td>2631979.0</td>\n",
       "      <td>13325897.0</td>\n",
       "      <td>32110762.680918</td>\n",
       "      <td>12581999.0</td>\n",
       "      <td>92941796.298751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>salmon</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3159.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3253.0</td>\n",
       "      <td>647701.0</td>\n",
       "      <td>7017078.0</td>\n",
       "      <td>4760472.294024</td>\n",
       "      <td>788253.0</td>\n",
       "      <td>4473418.087173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>salmon</td>\n",
       "      <td>360.0</td>\n",
       "      <td>4323.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4323.0</td>\n",
       "      <td>686618.0</td>\n",
       "      <td>7300266.0</td>\n",
       "      <td>4855674.562517</td>\n",
       "      <td>793087.0</td>\n",
       "      <td>4405833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>salmon</td>\n",
       "      <td>500.0</td>\n",
       "      <td>7713.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7713.0</td>\n",
       "      <td>700644.0</td>\n",
       "      <td>7334551.0</td>\n",
       "      <td>4189191.564075</td>\n",
       "      <td>731141.0</td>\n",
       "      <td>4386358.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>salmon</td>\n",
       "      <td>4630.875</td>\n",
       "      <td>3542.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3720.0</td>\n",
       "      <td>664036.0</td>\n",
       "      <td>7172574.0</td>\n",
       "      <td>4860118.271304</td>\n",
       "      <td>787082.0</td>\n",
       "      <td>4476651.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>low carb</td>\n",
       "      <td>2378.0</td>\n",
       "      <td>40544.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40544.0</td>\n",
       "      <td>2585867.0</td>\n",
       "      <td>11181066.0</td>\n",
       "      <td>26064293.944524</td>\n",
       "      <td>9212192.0</td>\n",
       "      <td>73580900.766766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>low carb</td>\n",
       "      <td>3283.0</td>\n",
       "      <td>38589.0</td>\n",
       "      <td>1345.0</td>\n",
       "      <td>101.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55397.0</td>\n",
       "      <td>2792590.0</td>\n",
       "      <td>13377807.148767</td>\n",
       "      <td>30890438.81955</td>\n",
       "      <td>11165841.0</td>\n",
       "      <td>86136270.603325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>salmon</td>\n",
       "      <td>1141.0</td>\n",
       "      <td>715.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>715.0</td>\n",
       "      <td>650860.0</td>\n",
       "      <td>7001796.0</td>\n",
       "      <td>4932979.56341</td>\n",
       "      <td>746614.0</td>\n",
       "      <td>4506666.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>294 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    theme_name_no additives/preservatives theme_name_salmon  \\\n",
       "0                                     0.0               1.0   \n",
       "1                                     0.0               1.0   \n",
       "2                                     0.0               0.0   \n",
       "3                                     0.0               1.0   \n",
       "4                                     0.0               1.0   \n",
       "..                                    ...               ...   \n",
       "289                                   0.0               1.0   \n",
       "290                                   0.0               1.0   \n",
       "291                                   0.0               0.0   \n",
       "292                                   0.0               0.0   \n",
       "293                                   0.0               1.0   \n",
       "\n",
       "    cat_variable_imputer_theme_name total_post google_searchVolume  \\\n",
       "0                            salmon      113.0              1700.0   \n",
       "1                            salmon     2649.0             12052.0   \n",
       "2                          low carb     1781.0             39684.0   \n",
       "3                            salmon      259.0              3159.0   \n",
       "4                            salmon      360.0              4323.0   \n",
       "..                              ...        ...                 ...   \n",
       "289                          salmon      500.0              7713.0   \n",
       "290                          salmon   4630.875              3542.0   \n",
       "291                        low carb     2378.0             40544.0   \n",
       "292                        low carb     3283.0             38589.0   \n",
       "293                          salmon     1141.0               715.0   \n",
       "\n",
       "    amazon_searchVolume chewy_searchVolume walmart_searchVolume  \\\n",
       "0                   0.0                0.0                  0.0   \n",
       "1                 532.0                0.0                  0.0   \n",
       "2                1345.0             101.25                  0.0   \n",
       "3                  94.0                0.0                  0.0   \n",
       "4                   0.0                0.0                  0.0   \n",
       "..                  ...                ...                  ...   \n",
       "289                 0.0                0.0                  0.0   \n",
       "290               178.0                0.0                  0.0   \n",
       "291                 0.0                0.0                  0.0   \n",
       "292              1345.0             101.25                  0.0   \n",
       "293                 0.0                0.0                  0.0   \n",
       "\n",
       "    total_searchVolume client_units_value client_dollars_value  \\\n",
       "0               1700.0           728731.0            6867987.0   \n",
       "1              12584.0           719860.0            7872415.0   \n",
       "2              49955.0          2631979.0           13325897.0   \n",
       "3               3253.0           647701.0            7017078.0   \n",
       "4               4323.0           686618.0            7300266.0   \n",
       "..                 ...                ...                  ...   \n",
       "289             7713.0           700644.0            7334551.0   \n",
       "290             3720.0           664036.0            7172574.0   \n",
       "291            40544.0          2585867.0           11181066.0   \n",
       "292            55397.0          2792590.0      13377807.148767   \n",
       "293              715.0           650860.0            7001796.0   \n",
       "\n",
       "    competitor_lbs_value competitor_units_value competitor_dollars_value  \n",
       "0              4499792.0               653720.0                4420861.0  \n",
       "1              4917567.0               784836.0           4494954.899164  \n",
       "2        32110762.680918             12581999.0          92941796.298751  \n",
       "3         4760472.294024               788253.0           4473418.087173  \n",
       "4         4855674.562517               793087.0                4405833.0  \n",
       "..                   ...                    ...                      ...  \n",
       "289       4189191.564075               731141.0                4386358.0  \n",
       "290       4860118.271304               787082.0                4476651.0  \n",
       "291      26064293.944524              9212192.0          73580900.766766  \n",
       "292       30890438.81955             11165841.0          86136270.603325  \n",
       "293        4932979.56341               746614.0                4506666.0  \n",
       "\n",
       "[294 rows x 14 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Univariate\n",
    "\n",
    "\n",
    "- Look at each variable independently. This is useful if your models have assumptions on the distribution and/or bounds on the features/target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['theme_name_no additives/preservatives', 'theme_name_salmon',\n",
       "       'cat_variable_imputer_theme_name', 'total_post', 'google_searchVolume',\n",
       "       'amazon_searchVolume', 'chewy_searchVolume', 'walmart_searchVolume',\n",
       "       'total_searchVolume', 'client_units_value', 'client_dollars_value',\n",
       "       'competitor_lbs_value', 'competitor_units_value',\n",
       "       'competitor_dollars_value'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove this , temporary code\n",
    "train_X = train_X.drop(columns=['cat_variable_imputer_theme_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['theme_name_no additives/preservatives', 'theme_name_salmon',\n",
       "       'total_post', 'google_searchVolume', 'amazon_searchVolume',\n",
       "       'chewy_searchVolume', 'walmart_searchVolume', 'total_searchVolume',\n",
       "       'client_units_value', 'client_dollars_value', 'competitor_lbs_value',\n",
       "       'competitor_units_value', 'competitor_dollars_value'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X['theme_name_no additives/preservatives'] = train_X['theme_name_no additives/preservatives'].astype(int)\n",
    "train_X['theme_name_salmon'] = train_X['theme_name_salmon'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in num_columns:\n",
    "    train_X[column] = train_X[column].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 294 entries, 0 to 293\n",
      "Data columns (total 13 columns):\n",
      " #   Column                                 Non-Null Count  Dtype  \n",
      "---  ------                                 --------------  -----  \n",
      " 0   theme_name_no additives/preservatives  294 non-null    int64  \n",
      " 1   theme_name_salmon                      294 non-null    int64  \n",
      " 2   total_post                             294 non-null    float64\n",
      " 3   google_searchVolume                    294 non-null    float64\n",
      " 4   amazon_searchVolume                    294 non-null    float64\n",
      " 5   chewy_searchVolume                     294 non-null    float64\n",
      " 6   walmart_searchVolume                   294 non-null    float64\n",
      " 7   total_searchVolume                     294 non-null    float64\n",
      " 8   client_units_value                     294 non-null    float64\n",
      " 9   client_dollars_value                   294 non-null    float64\n",
      " 10  competitor_lbs_value                   294 non-null    float64\n",
      " 11  competitor_units_value                 294 non-null    float64\n",
      " 12  competitor_dollars_value               294 non-null    float64\n",
      "dtypes: float64(11), int64(2)\n",
      "memory usage: 30.0 KB\n"
     ]
    }
   ],
   "source": [
    "train_X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = eda.get_density_plots(train_X, cols=train_X.columns.tolist())\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the plots are html\n",
    "reports.create_report({'univariate': out}, name='feature_analysis_univariate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports.feature_analysis(train_X,'./feature_analysis_report.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Bivariate - mutual interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find columns with high correlations and drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable 1</th>\n",
       "      <th>Variable 2</th>\n",
       "      <th>Corr Coef</th>\n",
       "      <th>Abs Corr Coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>competitor_dollars_value</td>\n",
       "      <td>competitor_lbs_value</td>\n",
       "      <td>0.998105</td>\n",
       "      <td>0.998105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>competitor_dollars_value</td>\n",
       "      <td>competitor_units_value</td>\n",
       "      <td>0.993704</td>\n",
       "      <td>0.993704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>competitor_lbs_value</td>\n",
       "      <td>competitor_units_value</td>\n",
       "      <td>0.987295</td>\n",
       "      <td>0.987295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>competitor_lbs_value</td>\n",
       "      <td>theme_name_salmon</td>\n",
       "      <td>-0.981824</td>\n",
       "      <td>0.981824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google_searchVolume</td>\n",
       "      <td>total_searchVolume</td>\n",
       "      <td>0.980723</td>\n",
       "      <td>0.980723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>competitor_dollars_value</td>\n",
       "      <td>theme_name_salmon</td>\n",
       "      <td>-0.979964</td>\n",
       "      <td>0.979964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>client_units_value</td>\n",
       "      <td>competitor_units_value</td>\n",
       "      <td>0.974897</td>\n",
       "      <td>0.974897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>client_units_value</td>\n",
       "      <td>competitor_dollars_value</td>\n",
       "      <td>0.962097</td>\n",
       "      <td>0.962097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>competitor_units_value</td>\n",
       "      <td>theme_name_salmon</td>\n",
       "      <td>-0.956849</td>\n",
       "      <td>0.956849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>client_units_value</td>\n",
       "      <td>competitor_lbs_value</td>\n",
       "      <td>0.955041</td>\n",
       "      <td>0.955041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>client_dollars_value</td>\n",
       "      <td>client_units_value</td>\n",
       "      <td>0.945492</td>\n",
       "      <td>0.945492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>client_dollars_value</td>\n",
       "      <td>competitor_units_value</td>\n",
       "      <td>0.919497</td>\n",
       "      <td>0.919497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>client_units_value</td>\n",
       "      <td>theme_name_salmon</td>\n",
       "      <td>-0.914903</td>\n",
       "      <td>0.914903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>amazon_searchVolume</td>\n",
       "      <td>chewy_searchVolume</td>\n",
       "      <td>0.906967</td>\n",
       "      <td>0.906967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>client_dollars_value</td>\n",
       "      <td>competitor_dollars_value</td>\n",
       "      <td>0.892769</td>\n",
       "      <td>0.892769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>client_dollars_value</td>\n",
       "      <td>competitor_lbs_value</td>\n",
       "      <td>0.888981</td>\n",
       "      <td>0.888981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>client_units_value</td>\n",
       "      <td>total_searchVolume</td>\n",
       "      <td>0.842621</td>\n",
       "      <td>0.842621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>competitor_units_value</td>\n",
       "      <td>total_searchVolume</td>\n",
       "      <td>0.841173</td>\n",
       "      <td>0.841173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>client_units_value</td>\n",
       "      <td>google_searchVolume</td>\n",
       "      <td>0.837001</td>\n",
       "      <td>0.837001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>client_dollars_value</td>\n",
       "      <td>total_searchVolume</td>\n",
       "      <td>0.825295</td>\n",
       "      <td>0.825295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>competitor_units_value</td>\n",
       "      <td>google_searchVolume</td>\n",
       "      <td>0.824997</td>\n",
       "      <td>0.824997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>competitor_dollars_value</td>\n",
       "      <td>total_searchVolume</td>\n",
       "      <td>0.803073</td>\n",
       "      <td>0.803073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>client_dollars_value</td>\n",
       "      <td>theme_name_salmon</td>\n",
       "      <td>-0.799546</td>\n",
       "      <td>0.799546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>competitor_dollars_value</td>\n",
       "      <td>google_searchVolume</td>\n",
       "      <td>0.790515</td>\n",
       "      <td>0.790515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>competitor_lbs_value</td>\n",
       "      <td>total_searchVolume</td>\n",
       "      <td>0.784155</td>\n",
       "      <td>0.784155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>client_dollars_value</td>\n",
       "      <td>google_searchVolume</td>\n",
       "      <td>0.782978</td>\n",
       "      <td>0.782978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>competitor_lbs_value</td>\n",
       "      <td>google_searchVolume</td>\n",
       "      <td>0.770256</td>\n",
       "      <td>0.770256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>google_searchVolume</td>\n",
       "      <td>theme_name_salmon</td>\n",
       "      <td>-0.737863</td>\n",
       "      <td>0.737863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>theme_name_salmon</td>\n",
       "      <td>total_searchVolume</td>\n",
       "      <td>-0.729675</td>\n",
       "      <td>0.729675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>amazon_searchVolume</td>\n",
       "      <td>total_searchVolume</td>\n",
       "      <td>0.650657</td>\n",
       "      <td>0.650657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>chewy_searchVolume</td>\n",
       "      <td>total_searchVolume</td>\n",
       "      <td>0.638174</td>\n",
       "      <td>0.638174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>chewy_searchVolume</td>\n",
       "      <td>client_dollars_value</td>\n",
       "      <td>0.628744</td>\n",
       "      <td>0.628744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>amazon_searchVolume</td>\n",
       "      <td>client_dollars_value</td>\n",
       "      <td>0.610723</td>\n",
       "      <td>0.610723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>total_post</td>\n",
       "      <td>total_searchVolume</td>\n",
       "      <td>0.609045</td>\n",
       "      <td>0.609045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Variable 1                Variable 2  Corr Coef  \\\n",
       "0   competitor_dollars_value      competitor_lbs_value   0.998105   \n",
       "1   competitor_dollars_value    competitor_units_value   0.993704   \n",
       "2       competitor_lbs_value    competitor_units_value   0.987295   \n",
       "3       competitor_lbs_value         theme_name_salmon  -0.981824   \n",
       "4        google_searchVolume        total_searchVolume   0.980723   \n",
       "5   competitor_dollars_value         theme_name_salmon  -0.979964   \n",
       "6         client_units_value    competitor_units_value   0.974897   \n",
       "7         client_units_value  competitor_dollars_value   0.962097   \n",
       "8     competitor_units_value         theme_name_salmon  -0.956849   \n",
       "9         client_units_value      competitor_lbs_value   0.955041   \n",
       "10      client_dollars_value        client_units_value   0.945492   \n",
       "11      client_dollars_value    competitor_units_value   0.919497   \n",
       "12        client_units_value         theme_name_salmon  -0.914903   \n",
       "13       amazon_searchVolume        chewy_searchVolume   0.906967   \n",
       "14      client_dollars_value  competitor_dollars_value   0.892769   \n",
       "15      client_dollars_value      competitor_lbs_value   0.888981   \n",
       "16        client_units_value        total_searchVolume   0.842621   \n",
       "17    competitor_units_value        total_searchVolume   0.841173   \n",
       "18        client_units_value       google_searchVolume   0.837001   \n",
       "19      client_dollars_value        total_searchVolume   0.825295   \n",
       "20    competitor_units_value       google_searchVolume   0.824997   \n",
       "21  competitor_dollars_value        total_searchVolume   0.803073   \n",
       "22      client_dollars_value         theme_name_salmon  -0.799546   \n",
       "23  competitor_dollars_value       google_searchVolume   0.790515   \n",
       "24      competitor_lbs_value        total_searchVolume   0.784155   \n",
       "25      client_dollars_value       google_searchVolume   0.782978   \n",
       "26      competitor_lbs_value       google_searchVolume   0.770256   \n",
       "27       google_searchVolume         theme_name_salmon  -0.737863   \n",
       "28         theme_name_salmon        total_searchVolume  -0.729675   \n",
       "29       amazon_searchVolume        total_searchVolume   0.650657   \n",
       "30        chewy_searchVolume        total_searchVolume   0.638174   \n",
       "31        chewy_searchVolume      client_dollars_value   0.628744   \n",
       "32       amazon_searchVolume      client_dollars_value   0.610723   \n",
       "33                total_post        total_searchVolume   0.609045   \n",
       "\n",
       "    Abs Corr Coef  \n",
       "0        0.998105  \n",
       "1        0.993704  \n",
       "2        0.987295  \n",
       "3        0.981824  \n",
       "4        0.980723  \n",
       "5        0.979964  \n",
       "6        0.974897  \n",
       "7        0.962097  \n",
       "8        0.956849  \n",
       "9        0.955041  \n",
       "10       0.945492  \n",
       "11       0.919497  \n",
       "12       0.914903  \n",
       "13       0.906967  \n",
       "14       0.892769  \n",
       "15       0.888981  \n",
       "16       0.842621  \n",
       "17       0.841173  \n",
       "18       0.837001  \n",
       "19       0.825295  \n",
       "20       0.824997  \n",
       "21       0.803073  \n",
       "22       0.799546  \n",
       "23       0.790515  \n",
       "24       0.784155  \n",
       "25       0.782978  \n",
       "26       0.770256  \n",
       "27       0.737863  \n",
       "28       0.729675  \n",
       "29       0.650657  \n",
       "30       0.638174  \n",
       "31       0.628744  \n",
       "32       0.610723  \n",
       "33       0.609045  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = eda.get_correlation_table(train_X)\n",
    "out[out[\"Abs Corr Coef\"] > 0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable 1</th>\n",
       "      <th>Variable 2</th>\n",
       "      <th>Corr Coef</th>\n",
       "      <th>Abs Corr Coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>competitor_dollars_value</td>\n",
       "      <td>competitor_lbs_value</td>\n",
       "      <td>0.998105</td>\n",
       "      <td>0.998105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>competitor_dollars_value</td>\n",
       "      <td>competitor_units_value</td>\n",
       "      <td>0.993704</td>\n",
       "      <td>0.993704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>competitor_lbs_value</td>\n",
       "      <td>competitor_units_value</td>\n",
       "      <td>0.987295</td>\n",
       "      <td>0.987295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>competitor_lbs_value</td>\n",
       "      <td>theme_name_salmon</td>\n",
       "      <td>-0.981824</td>\n",
       "      <td>0.981824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>competitor_dollars_value</td>\n",
       "      <td>theme_name_salmon</td>\n",
       "      <td>-0.979964</td>\n",
       "      <td>0.979964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>client_units_value</td>\n",
       "      <td>competitor_units_value</td>\n",
       "      <td>0.974897</td>\n",
       "      <td>0.974897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>client_units_value</td>\n",
       "      <td>competitor_dollars_value</td>\n",
       "      <td>0.962097</td>\n",
       "      <td>0.962097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>competitor_units_value</td>\n",
       "      <td>theme_name_salmon</td>\n",
       "      <td>-0.956849</td>\n",
       "      <td>0.956849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>client_units_value</td>\n",
       "      <td>competitor_lbs_value</td>\n",
       "      <td>0.955041</td>\n",
       "      <td>0.955041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>client_dollars_value</td>\n",
       "      <td>client_units_value</td>\n",
       "      <td>0.945492</td>\n",
       "      <td>0.945492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>client_dollars_value</td>\n",
       "      <td>competitor_units_value</td>\n",
       "      <td>0.919497</td>\n",
       "      <td>0.919497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>client_units_value</td>\n",
       "      <td>theme_name_salmon</td>\n",
       "      <td>-0.914903</td>\n",
       "      <td>0.914903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>client_dollars_value</td>\n",
       "      <td>competitor_dollars_value</td>\n",
       "      <td>0.892769</td>\n",
       "      <td>0.892769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>client_dollars_value</td>\n",
       "      <td>competitor_lbs_value</td>\n",
       "      <td>0.888981</td>\n",
       "      <td>0.888981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>client_units_value</td>\n",
       "      <td>google_searchVolume</td>\n",
       "      <td>0.837001</td>\n",
       "      <td>0.837001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>competitor_units_value</td>\n",
       "      <td>google_searchVolume</td>\n",
       "      <td>0.824997</td>\n",
       "      <td>0.824997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>client_dollars_value</td>\n",
       "      <td>theme_name_salmon</td>\n",
       "      <td>-0.799546</td>\n",
       "      <td>0.799546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>competitor_dollars_value</td>\n",
       "      <td>google_searchVolume</td>\n",
       "      <td>0.790515</td>\n",
       "      <td>0.790515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>client_dollars_value</td>\n",
       "      <td>google_searchVolume</td>\n",
       "      <td>0.782978</td>\n",
       "      <td>0.782978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>competitor_lbs_value</td>\n",
       "      <td>google_searchVolume</td>\n",
       "      <td>0.770256</td>\n",
       "      <td>0.770256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>google_searchVolume</td>\n",
       "      <td>theme_name_salmon</td>\n",
       "      <td>-0.737863</td>\n",
       "      <td>0.737863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>amazon_searchVolume</td>\n",
       "      <td>client_dollars_value</td>\n",
       "      <td>0.610723</td>\n",
       "      <td>0.610723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Variable 1                Variable 2  Corr Coef  \\\n",
       "0   competitor_dollars_value      competitor_lbs_value   0.998105   \n",
       "1   competitor_dollars_value    competitor_units_value   0.993704   \n",
       "2       competitor_lbs_value    competitor_units_value   0.987295   \n",
       "3       competitor_lbs_value         theme_name_salmon  -0.981824   \n",
       "4   competitor_dollars_value         theme_name_salmon  -0.979964   \n",
       "5         client_units_value    competitor_units_value   0.974897   \n",
       "6         client_units_value  competitor_dollars_value   0.962097   \n",
       "7     competitor_units_value         theme_name_salmon  -0.956849   \n",
       "8         client_units_value      competitor_lbs_value   0.955041   \n",
       "9       client_dollars_value        client_units_value   0.945492   \n",
       "10      client_dollars_value    competitor_units_value   0.919497   \n",
       "11        client_units_value         theme_name_salmon  -0.914903   \n",
       "12      client_dollars_value  competitor_dollars_value   0.892769   \n",
       "13      client_dollars_value      competitor_lbs_value   0.888981   \n",
       "14        client_units_value       google_searchVolume   0.837001   \n",
       "15    competitor_units_value       google_searchVolume   0.824997   \n",
       "16      client_dollars_value         theme_name_salmon  -0.799546   \n",
       "17  competitor_dollars_value       google_searchVolume   0.790515   \n",
       "18      client_dollars_value       google_searchVolume   0.782978   \n",
       "19      competitor_lbs_value       google_searchVolume   0.770256   \n",
       "20       google_searchVolume         theme_name_salmon  -0.737863   \n",
       "21       amazon_searchVolume      client_dollars_value   0.610723   \n",
       "\n",
       "    Abs Corr Coef  \n",
       "0        0.998105  \n",
       "1        0.993704  \n",
       "2        0.987295  \n",
       "3        0.981824  \n",
       "4        0.979964  \n",
       "5        0.974897  \n",
       "6        0.962097  \n",
       "7        0.956849  \n",
       "8        0.955041  \n",
       "9        0.945492  \n",
       "10       0.919497  \n",
       "11       0.914903  \n",
       "12       0.892769  \n",
       "13       0.888981  \n",
       "14       0.837001  \n",
       "15       0.824997  \n",
       "16       0.799546  \n",
       "17       0.790515  \n",
       "18       0.782978  \n",
       "19       0.770256  \n",
       "20       0.737863  \n",
       "21       0.610723  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping total search volume as its highly correlated with google search volume\n",
    "# Dropping chewy search volume as its highly correlated with amazon search volume\n",
    "# We will be going forward with the rest of the features as they represent key \n",
    "# features to account for after the modelling stage(despite the high correlations).\n",
    "curated_columns = list(\n",
    "    set(train_X.columns.to_list()) \n",
    "    - set(['total_searchVolume', 'chewy_searchVolume'])\n",
    ")\n",
    "\n",
    "train_X = train_X[curated_columns]\n",
    "\n",
    "out = eda.get_correlation_table(train_X)\n",
    "out[out[\"Abs Corr Coef\"] > 0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = eda.get_bivariate_plots(train_X, x_cols=['theme_name_salmon'], y_cols=['theme_name_no additives/preservatives'])\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create reports as needed\n",
    "cols = train_X.columns.to_list()\n",
    "all_plots = {}\n",
    "for ii, col1 in enumerate(cols): \n",
    "    for jj in range(ii+1, len(cols)):\n",
    "        col2 = cols[jj]\n",
    "        out = eda.get_bivariate_plots(train_X, x_cols=[col1], y_cols=[col2])\n",
    "        all_plots.update({f'{col2} vs {col1}': out})\n",
    "\n",
    "reports.create_report(all_plots, name='feature_analysis_bivariate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports.feature_interactions(train_X,'./feature_interaction_report.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Key Drivers - Interaction with Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = eda.get_target_correlation(train_X, train_y, y_continuous=True)\n",
    "display_as_tabs([(k, v) for k,v in out.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y['client_lbs_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = eda.get_feature_importances(train_X, train_y, y_continuous=True)\n",
    "display_as_tabs([(k, v) for k,v in out.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key drivers report like feature importance, bivariate plots can be obtained as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports.key_drivers(train_X,train_y, './key_drivers_report.html', y_continuous=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dev Notes**\n",
    "<details>\n",
    "    \n",
    "- The SHAP plots and bivariate plots in key drivers reports can be obtained by including quick=False as a parameter to key_drivers function call. \n",
    "- SHAP plots and bivariate plots often take long depending on data shape.\n",
    "- The plot with shap is present [here](https://drive.google.com/file/d/1JOTMBLiv3LEqZ-kxZz0RokW9v5UyiGva/view?usp=sharing)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "All the plots like feature analysis, interaction, key drivers can be obtained as a single plot using data exploration method as shown below. The output from this is available [here](https://drive.google.com/file/d/1209MzmSSEhiTYuPfHpaVXFXUVbkaJm0B/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports.data_exploration(train_X,train_y,'./data_exploration_report.html', y_continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the list of relevant columns\n",
    "save_pipeline(curated_columns, op.abspath(op.join(artifacts_folder, 'curated_columns.joblib')))\n",
    "\n",
    "# save the feature pipeline\n",
    "save_pipeline(features_transformer, op.abspath(op.join(artifacts_folder, 'features.joblib')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Modelling - Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Feature Selection(Specific to Regression)\n",
    "\n",
    "- Selecting Features specific to regression\n",
    "- VIF : measure of the amount of multi-collinearity in a set of multiple regressor variables. \n",
    "- On a case to case basis VIF thresholds change. Generally 5 or 10 are acceptable levels.\n",
    "- Usually on a recursive basis when removing the most collinear variable, there can be shuffle in VIF. \n",
    "- Often this section will not be part of the production code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make sure client and competitor features are not getting removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid value encountered in double_scalars\n",
      "invalid value encountered in double_scalars\n",
      "invalid value encountered in double_scalars\n",
      "invalid value encountered in double_scalars\n",
      "invalid value encountered in double_scalars\n",
      "invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "cols = list(train_X.columns)\n",
    "vif = eda.calc_vif(train_X)\n",
    "while max(vif.VIF) > 12:\n",
    "    #removing the largest variable from VIF\n",
    "    cols.remove(vif[(vif.VIF==vif.VIF.max())].variables.tolist()[0])\n",
    "    vif = eda.calc_vif(train_X[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_vars = vif.query('VIF < 11').variables\n",
    "reg_vars = list(reg_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['google_searchVolume',\n",
       " 'total_post',\n",
       " 'amazon_searchVolume',\n",
       " 'theme_name_no additives/preservatives',\n",
       " 'theme_name_salmon']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transformations like these can be utilised\n",
    "def _custom_data_transform(df, cols2keep=None):\n",
    "    \"\"\"Transformation to drop some columns in the data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        df - pd.DataFrame\n",
    "        cols2keep - columns to keep in the dataframe\n",
    "    \"\"\"\n",
    "    cols2keep = cols2keep or []\n",
    "    if len(cols2keep):\n",
    "        return (df\n",
    "                .select_columns(cols2keep))\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Model training pipeline\n",
    "\n",
    "- Here we focus on creating a collection of pipelines that can be used for training respective models.\n",
    "- Each model pipeline will essentially be of the form\n",
    "```\n",
    "[\n",
    "('preprocessing', preprocessing_pipeline),\n",
    "('feature_selection', feature_selection_pipeline),\n",
    "('estimator', estimator),\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Add feature engineering section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 Model Pipeline Build\n",
    "\n",
    "- This will be part of the production code (training only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;&#x27;,\n",
       "                 FunctionTransformer(func=&lt;function _custom_data_transform at 0x7f7fef101160&gt;,\n",
       "                                     kw_args={&#x27;cols2keep&#x27;: [&#x27;google_searchVolume&#x27;,\n",
       "                                                            &#x27;total_post&#x27;,\n",
       "                                                            &#x27;amazon_searchVolume&#x27;,\n",
       "                                                            &#x27;theme_name_no &#x27;\n",
       "                                                            &#x27;additives/preservatives&#x27;,\n",
       "                                                            &#x27;theme_name_salmon&#x27;]})),\n",
       "                (&#x27;estimator&#x27;, SKLStatsmodelOLS())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;&#x27;,\n",
       "                 FunctionTransformer(func=&lt;function _custom_data_transform at 0x7f7fef101160&gt;,\n",
       "                                     kw_args={&#x27;cols2keep&#x27;: [&#x27;google_searchVolume&#x27;,\n",
       "                                                            &#x27;total_post&#x27;,\n",
       "                                                            &#x27;amazon_searchVolume&#x27;,\n",
       "                                                            &#x27;theme_name_no &#x27;\n",
       "                                                            &#x27;additives/preservatives&#x27;,\n",
       "                                                            &#x27;theme_name_salmon&#x27;]})),\n",
       "                (&#x27;estimator&#x27;, SKLStatsmodelOLS())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;function _custom_data_transform at 0x7f7fef101160&gt;,\n",
       "                    kw_args={&#x27;cols2keep&#x27;: [&#x27;google_searchVolume&#x27;, &#x27;total_post&#x27;,\n",
       "                                           &#x27;amazon_searchVolume&#x27;,\n",
       "                                           &#x27;theme_name_no &#x27;\n",
       "                                           &#x27;additives/preservatives&#x27;,\n",
       "                                           &#x27;theme_name_salmon&#x27;]})</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SKLStatsmodelOLS</label><div class=\"sk-toggleable__content\"><pre>SKLStatsmodelOLS()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('',\n",
       "                 FunctionTransformer(func=<function _custom_data_transform at 0x7f7fef101160>,\n",
       "                                     kw_args={'cols2keep': ['google_searchVolume',\n",
       "                                                            'total_post',\n",
       "                                                            'amazon_searchVolume',\n",
       "                                                            'theme_name_no '\n",
       "                                                            'additives/preservatives',\n",
       "                                                            'theme_name_salmon']})),\n",
       "                ('estimator', SKLStatsmodelOLS())])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_ppln_ols = Pipeline([\n",
    "    ('',FunctionTransformer(_custom_data_transform, kw_args={'cols2keep':reg_vars})),\n",
    "    ('estimator', SKLStatsmodelOLS())\n",
    "])\n",
    "reg_ppln_ols.fit(train_X, train_y.values.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.801</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.797</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   231.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 09 Jun 2023</td> <th>  Prob (F-statistic):</th> <td>1.43e-98</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:27:21</td>     <th>  Log-Likelihood:    </th> <td> -4203.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   294</td>      <th>  AIC:               </th> <td>   8419.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   288</td>      <th>  BIC:               </th> <td>   8441.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                    <td></td>                       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th>                             <td> 3.995e+06</td> <td> 1.28e+05</td> <td>   31.128</td> <td> 0.000</td> <td> 3.74e+06</td> <td> 4.25e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>google_searchVolume</th>                   <td>  -10.6856</td> <td>    2.691</td> <td>   -3.971</td> <td> 0.000</td> <td>  -15.982</td> <td>   -5.390</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>total_post</th>                            <td>  117.1397</td> <td>   25.529</td> <td>    4.589</td> <td> 0.000</td> <td>   66.893</td> <td>  167.386</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>amazon_searchVolume</th>                   <td>  450.9269</td> <td>   51.560</td> <td>    8.746</td> <td> 0.000</td> <td>  349.445</td> <td>  552.409</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>theme_name_no additives/preservatives</th> <td>-8.455e+05</td> <td> 1.53e+05</td> <td>   -5.542</td> <td> 0.000</td> <td>-1.15e+06</td> <td>-5.45e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>theme_name_salmon</th>                     <td> 1.224e+06</td> <td> 1.11e+05</td> <td>   11.007</td> <td> 0.000</td> <td> 1.01e+06</td> <td> 1.44e+06</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>29.448</td> <th>  Durbin-Watson:     </th> <td>   1.972</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  36.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.760</td> <th>  Prob(JB):          </th> <td>1.45e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.799</td> <th>  Cond. No.          </th> <td>3.07e+05</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 3.07e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.801\n",
       "Model:                            OLS   Adj. R-squared:                  0.797\n",
       "Method:                 Least Squares   F-statistic:                     231.2\n",
       "Date:                Fri, 09 Jun 2023   Prob (F-statistic):           1.43e-98\n",
       "Time:                        16:27:21   Log-Likelihood:                -4203.6\n",
       "No. Observations:                 294   AIC:                             8419.\n",
       "Df Residuals:                     288   BIC:                             8441.\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=========================================================================================================\n",
       "                                            coef    std err          t      P>|t|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------------------------\n",
       "intercept                              3.995e+06   1.28e+05     31.128      0.000    3.74e+06    4.25e+06\n",
       "google_searchVolume                     -10.6856      2.691     -3.971      0.000     -15.982      -5.390\n",
       "total_post                              117.1397     25.529      4.589      0.000      66.893     167.386\n",
       "amazon_searchVolume                     450.9269     51.560      8.746      0.000     349.445     552.409\n",
       "theme_name_no additives/preservatives -8.455e+05   1.53e+05     -5.542      0.000   -1.15e+06   -5.45e+05\n",
       "theme_name_salmon                      1.224e+06   1.11e+05     11.007      0.000    1.01e+06    1.44e+06\n",
       "==============================================================================\n",
       "Omnibus:                       29.448   Durbin-Watson:                   1.972\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               36.091\n",
       "Skew:                           0.760   Prob(JB):                     1.45e-08\n",
       "Kurtosis:                       3.799   Cond. No.                     3.07e+05\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 3.07e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_ppln_ols['estimator'].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.5 Model Evaluation(Linear Model)\n",
    "\n",
    "This will be part of the production code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ppln = Pipeline([\n",
    "    ('', FunctionTransformer(_custom_data_transform, kw_args={'cols2keep':reg_vars})),\n",
    "    ('Linear Regression', SKLStatsmodelOLS())\n",
    "])\n",
    "\n",
    "test_X = get_dataframe(\n",
    "    features_transformer.transform(test_X), \n",
    "    get_feature_names_from_column_transformer(features_transformer)\n",
    ")\n",
    "test_X = test_X[curated_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_linear_report = RegressionReport(model=reg_ppln, x_train=train_X, y_train=train_y, x_test= test_X, y_test= test_y, refit=True)\n",
    "reg_linear_report.get_report(include_shap=False, file_path='regression_linear_model_report')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dev Notes**\n",
    "Use SHAP for variable interpretability.\n",
    "<details>\n",
    "\n",
    "    1. Use SHAP=True to generate variable interpretability plots in the report\n",
    "    2. SHAP is recommended for non parameteric models such as RF, xgboost.\n",
    "    3. However, SHAP reports are time consuming depending on no.of records and model complexity.\n",
    "    \n",
    "A sample of regerssion report with SHAP can be found [here](https://drive.google.com/file/d/18RlQTsT1ze09Cgz-qpb4ha_cvyWbN5F5/view?usp=sharing).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.6 Residual Analysis\n",
    "- After scoring the model, it is recommended to do a residual analysis to know the distribution of errors\n",
    "- we took a threshold of 30% above which it is marked as over prediction or underprediction\n",
    "- This will not be part of the production code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.3\n",
    "residual_analysis = test_X.copy()\n",
    "residual_analysis['prediction'] = reg_ppln_ols.predict(test_X)\n",
    "residual_analysis['actuals'] = test_y.reset_index(drop = True).iloc[:,0].values\n",
    "residual_analysis['forecast_flag'] = 'good'\n",
    "residual_analysis.loc[((residual_analysis['prediction'] > (1+threshold) * residual_analysis['actuals'])\\\n",
    "                       & (residual_analysis['actuals']>100)),'forecast_flag'] = 'over predict'\n",
    "residual_analysis.loc[((residual_analysis['prediction'] < (1-threshold) * residual_analysis['actuals'])\\\n",
    "                       & (residual_analysis['actuals']>100)),'forecast_flag'] = 'under predict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_analysis.hvplot.kde(y=\"client_lbs_value\",by=\"forecast_flag\", ## Grouping by Predictions\n",
    "                                width=800, height=400,\n",
    "                                alpha=0.7,\n",
    "                                ylabel=\"density\",\n",
    "                                xlabel=\"unit_cost\",\n",
    "                                title=f'unit cost(density)',legend='top_right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the above plot we can infer that the higher \"over predictions\" are happening for unit_cost > 200.\n",
    "- similarly, the higher \"under predictions\" are happening for unit_cost is zero.\n",
    "\n",
    "This can help us tune the model by a separate model for unit_cost > 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Modelling - XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.1 Model training pipeline\n",
    "\n",
    "Here we focus on creating a collection of pipelines that can be used for tranining respective models.\n",
    "\n",
    "Each model pipeline will essentially be of the form\n",
    "```\n",
    "[\n",
    "('preprocessing', preprocessing_pipeline),\n",
    "('feature_selection', feature_selection_pipeline),\n",
    "('estimator', estimator),\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Model Pipeline Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's find features for some decent defaults\n",
    "estimator = XGBRegressor()\n",
    "xgb_training_pipe_init = Pipeline([\n",
    "    ('XGBoost', XGBRegressor())\n",
    "])\n",
    "xgb_training_pipe_init.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding the Feature Importance\n",
    "%matplotlib inline\n",
    "imp = pd.DataFrame({'importance': xgb_training_pipe_init['XGBoost'].feature_importances_})\n",
    "imp.index = train_X.columns\n",
    "imp.sort_values('importance',inplace=True)\n",
    "imp.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'condition','model_family','days_since_last_purchase','first_time_customer','sales_person', are considered to be important and in grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pipeline build based on new importance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's find features for some decent defaults\n",
    "imp_features = ['model_family','sku','unit_cost','condition','brand','business_unit']\n",
    "\n",
    "estimator = XGBRegressor()\n",
    "xgb_training_pipe2 = Pipeline([\n",
    "    ('', FunctionTransformer(_custom_data_transform, kw_args={'cols2keep':imp_features})),\n",
    "    ('XGBoost', XGBRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search of the Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "parameters = {\n",
    "   'gamma':[0.03],\n",
    "   'min_child_weight':[6],\n",
    "   'learning_rate':[0.1],\n",
    "   'max_depth':[3],\n",
    "   'n_estimators':[500], \n",
    "}\n",
    "est = XGBRegressor()\n",
    "xgb_grid = GridSearchCV(est,\n",
    "                        parameters,\n",
    "                        cv = 2,\n",
    "                        n_jobs = 4,\n",
    "                        verbose=True)\n",
    "\n",
    "xgb_grid.fit(train_X, train_y)\n",
    "\n",
    "print(xgb_grid.best_score_)\n",
    "print(xgb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Build using the best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pipeline_final = Pipeline([\n",
    "    ('', FunctionTransformer(_custom_data_transform, kw_args={'cols2keep':imp_features})),\n",
    "    ('XGBoost', xgb_grid.best_estimator_)\n",
    "])\n",
    "xgb_pipeline_final.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_tree_report = RegressionReport(model=xgb_pipeline_final, x_train=train_X, y_train=train_y, x_test= test_X, y_test= test_y)\n",
    "reg_tree_report.get_report(include_shap=False, file_path='regression_tree_model_report')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Regression report containing the feature importances are available [here](https://drive.google.com/file/d/1JBfL3uxPcxBfl0amweXBFmLr7CSHFBUO/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a comparison report of the  linear (vs) tree -based model  approach can be generated as follows.\n",
    "\n",
    "This code will not be part of the production code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipelines = [reg_ppln, xgb_pipeline_final]\n",
    "model_comparison_report = RegressionComparison(models=model_pipelines,x=train_X, y=train_y)\n",
    "metrics = model_comparison_report.get_report(file_path='regression_comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison_report.performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A report comparing the performance, metrics between Linear model and Tree model are available [here](https://drive.google.com/file/d/1LDibiFap9K4DKME-Y0S0mtI_05lTdaJF/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dev NOTES**\n",
    "<details>\n",
    "\n",
    "the above metrics are absolute nos and not %ges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we are choosing LM model for pipelining. General criteria for choosing production models is:\n",
    "\n",
    "- Parametric models (aka whitebox models) such as Linear Regression are easier to explain to non-technical audience.\n",
    "- Generally these are accepted fast and adoption is quicker.\n",
    "- If the downstream calls for optimization using these models parametric models are easier to implement.\n",
    "- When accuracy is primary goal without explainability, the above two takes a backseat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
