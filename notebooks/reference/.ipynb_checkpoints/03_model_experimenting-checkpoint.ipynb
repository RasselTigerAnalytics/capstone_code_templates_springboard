{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the model experimentation and finalization. It covers EDA, outlier treatment, transformation, training, model evaluation and comparison across models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import shutil\n",
    "\n",
    "# standard third party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# impute missing values\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import KNNImputer, IterativeImputer, SimpleImputer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', message=\"pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\", \n",
    "                        category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', message=\"pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\",\n",
    "                        category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard code-template imports\n",
    "from ta_lib.core.api import (\n",
    "    create_context, get_dataframe, get_feature_names_from_column_transformer, string_cleaning,\n",
    "    get_package_path, display_as_tabs, save_pipeline, load_pipeline, initialize_environment,\n",
    "    load_dataset, save_dataset, DEFAULT_ARTIFACTS_PATH, list_datasets\n",
    ")\n",
    "\n",
    "import ta_lib.eda.api as eda\n",
    "from xgboost import XGBRegressor\n",
    "from ta_lib.regression.api import SKLStatsmodelOLS\n",
    "from ta_lib.regression.api import RegressionComparison, RegressionReport\n",
    "import ta_lib.reports.api as reports\n",
    "from ta_lib.data_processing.api import Outlier\n",
    "\n",
    "initialize_environment(debug=False, hide_warnings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts_folder = DEFAULT_ARTIFACTS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = op.join('conf', 'config.yml')\n",
    "context = create_context(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/raw/orders',\n",
       " '/raw/product',\n",
       " '/cleaned/orders',\n",
       " '/cleaned/product',\n",
       " '/cleaned/sales',\n",
       " '/processed/google_search',\n",
       " '/processed/sales',\n",
       " '/processed/social_media',\n",
       " '/processed/ground_truth',\n",
       " '/train/sales/features',\n",
       " '/train/sales/target',\n",
       " '/test/sales/features',\n",
       " '/test/sales/target',\n",
       " '/score/sales/output']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_datasets(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_search_data = load_dataset(context,  '/processed/google_search')\n",
    "sales_data = load_dataset(context, '/processed/sales')\n",
    "social_media_data = load_dataset(context, '/processed/social_media')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 EDA/Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Modularize the whole code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_ground_truth = sales_data.copy()\n",
    "social_media_ground_truth = social_media_data.copy()\n",
    "google_search_data_ground_truth = google_search_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_out_common_themes(dataframe: pd.DataFrame) -> tuple:\n",
    "    \"\"\"\n",
    "    Filters out the common themes.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    dataframe : pd.DataFrame\n",
    "        The dataframe to filter out the themes from.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple of dataframes.\n",
    "    \"\"\"\n",
    "    common_theme_list = set(sales_ground_truth['theme_name']) & set(social_media_ground_truth['theme_name']) & set(google_search_data_ground_truth['theme_name'])\n",
    "    \n",
    "    temp_dataframe = dataframe[dataframe['theme_name'].isin(common_theme_list)]\n",
    "    \n",
    "    return temp_dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_ground_truth = filter_out_common_themes(sales_ground_truth)\n",
    "social_media_ground_truth = filter_out_common_themes(social_media_ground_truth)\n",
    "google_search_data_ground_truth = filter_out_common_themes(google_search_data_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_ground_truth['theme_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_3_themes_by_sales_in_lbs(dataframe: pd.DataFrame) -> list:\n",
    "    \"\"\"\n",
    "    Fetches the top 3 sales by lbs.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    dataframe\n",
    "        The dataframe to use for getting the top 3 sales.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of the top 3 themes.\n",
    "    \"\"\"\n",
    "    \n",
    "    temp_dataframe = dataframe.copy()\n",
    "\n",
    "    total_sales_data = temp_dataframe[['date', 'theme_name', 'sales_lbs_value']]\n",
    "\n",
    "    total_sales_data = total_sales_data[total_sales_data['theme_name'] != 'No Claim']\n",
    "    total_sales_data['total_sales_by_theme'] = total_sales_data.groupby(['theme_name'])['sales_lbs_value'].transform('sum')\n",
    "\n",
    "    total_sales_data = total_sales_data[['theme_name', 'total_sales_by_theme']]\n",
    "    total_sales_data.drop_duplicates(inplace=True)\n",
    "\n",
    "    # We are taking the top 3 themes by sales in lbs\n",
    "    total_sales_data.sort_values(by=['total_sales_by_theme'], ascending=False, inplace=True)\n",
    "\n",
    "    total_sales_data = total_sales_data[:3]\n",
    "\n",
    "    top_themes = total_sales_data['theme_name'].unique().tolist()\n",
    "    \n",
    "    return top_themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_themes = get_top_3_themes_by_sales_in_lbs(sales_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['low carb', 'no additives/preservatives', 'salmon']\n"
     ]
    }
   ],
   "source": [
    "print(top_themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_out_top_three_themes(dataframe_list : pd.DataFrame, theme_list: list) -> None:\n",
    "    \"\"\"\n",
    "    Filter out the top 3 themes from the dataframes provided.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    dataframe_list : list[pd.DataFrame]\n",
    "        List of dataframes to apply the filtering on.\n",
    "        \n",
    "    theme_list : list\n",
    "        The list of themes to use for filtering.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    for dataframe in dataframe_list:\n",
    "        dataframe = dataframe[dataframe['theme_name'].isin(theme_list)]\n",
    "    print('Dataframes filtered successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes filtered successfully!\n"
     ]
    }
   ],
   "source": [
    "filter_out_top_three_themes([sales_ground_truth, social_media_ground_truth, google_search_data_ground_truth], top_themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "social_media_ground_truth['theme_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking only the top 3 themes\n",
    "sales_ground_truth = sales_ground_truth[sales_ground_truth['theme_name'].isin(top_themes)]\n",
    "social_media_ground_truth = social_media_ground_truth[social_media_ground_truth['theme_name'].isin(top_themes)]\n",
    "google_search_data_ground_truth = google_search_data_ground_truth[google_search_data_ground_truth['theme_name'].isin(top_themes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(632, 9)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_search_data_ground_truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_google_search_data(google_search_ground_truth: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregates the google search data.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    google_search_ground_truth : pd.DataFrame\n",
    "        The google search dataframe to aggregate.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The aggregated dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    temp_dataframe = google_search_data.copy()\n",
    "    required_cols = ['date','theme_name'\n",
    "     , 'google_searchVolume', 'amazon_searchVolume', 'chewy_searchVolume', 'walmart_searchVolume', 'total_searchVolume']\n",
    "    temp_dataframe = temp_dataframe.filter(required_cols)\n",
    "\n",
    "\n",
    "    temp_dataframe.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return temp_dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_search_data_ground_truth = aggregate_google_search_data(google_search_data_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28600, 7)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_search_data_ground_truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_social_media_data(social_media_ground_truth: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregates the google search data.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    social_media_ground_truth : pd.DataFrame\n",
    "        The social media data to aggregate.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The aggregated dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    temp_dataframe = social_media_ground_truth.copy()\n",
    "    required_cols = ['date', 'theme_name', 'total_post']\n",
    "\n",
    "    temp_dataframe = temp_dataframe.filter(required_cols)\n",
    "\n",
    "\n",
    "    temp_dataframe.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return temp_dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_media_ground_truth = aggregate_social_media_data(social_media_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>theme_name</th>\n",
       "      <th>total_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>2018-12-15</td>\n",
       "      <td>low carb</td>\n",
       "      <td>2531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>2018-10-13</td>\n",
       "      <td>low carb</td>\n",
       "      <td>2096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2015-06-20</td>\n",
       "      <td>low carb</td>\n",
       "      <td>913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2015-10-17</td>\n",
       "      <td>low carb</td>\n",
       "      <td>1506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9555</th>\n",
       "      <td>2018-10-27</td>\n",
       "      <td>salmon</td>\n",
       "      <td>1935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9553</th>\n",
       "      <td>2018-10-13</td>\n",
       "      <td>salmon</td>\n",
       "      <td>603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1288</th>\n",
       "      <td>2016-03-26</td>\n",
       "      <td>no additives/preservatives</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9583</th>\n",
       "      <td>2019-05-11</td>\n",
       "      <td>salmon</td>\n",
       "      <td>981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>no additives/preservatives</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2015-09-19</td>\n",
       "      <td>low carb</td>\n",
       "      <td>1809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date                  theme_name  total_post\n",
       "196   2018-12-15                    low carb        2531\n",
       "187   2018-10-13                    low carb        2096\n",
       "14    2015-06-20                    low carb         913\n",
       "31    2015-10-17                    low carb        1506\n",
       "9555  2018-10-27                      salmon        1935\n",
       "9553  2018-10-13                      salmon         603\n",
       "1288  2016-03-26  no additives/preservatives          37\n",
       "9583  2019-05-11                      salmon         981\n",
       "1454  2019-06-01  no additives/preservatives         226\n",
       "27    2015-09-19                    low carb        1809"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "social_media_ground_truth.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_sales_data(sales_ground_truth: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate the sales data.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    sales_ground_truth : pd.DataFrame\n",
    "        The sales data to aggregate.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The aggregated sales data.\n",
    "    \"\"\"\n",
    "    temp_dataframe = sales_ground_truth.copy()\n",
    "    temp_dataframe['date'] = pd.to_datetime(temp_dataframe['date'])\n",
    "\n",
    "\n",
    "    required_cols = ['date', 'sales_dollars_value', 'sales_units_value', 'sales_lbs_value', 'vendor', 'theme_name']\n",
    "    temp_dataframe = temp_dataframe.filter(required_cols)\n",
    "\n",
    "    temp_dataframe['weekly_lbs_value'] = temp_dataframe.groupby(['date', 'theme_name', 'vendor'])['sales_lbs_value'].transform('sum')\n",
    "    temp_dataframe['weekly_units_value'] = temp_dataframe.groupby(['date', 'theme_name', 'vendor'])['sales_units_value'].transform('sum')\n",
    "    temp_dataframe['weekly_dollars_value'] = temp_dataframe.groupby(['date', 'theme_name', 'vendor'])['sales_dollars_value'].transform('sum')\n",
    "    temp_dataframe = temp_dataframe[['date', 'theme_name', 'vendor', 'weekly_lbs_value', 'weekly_units_value', 'weekly_dollars_value']]\n",
    "    temp_dataframe.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return temp_dataframe\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_ground_truth = aggregate_sales_data(sales_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date_columns_to_datetime():\n",
    "    sales_ground_truth['date'] = pd.to_datetime(sales_ground_truth['date'])\n",
    "    social_media_ground_truth['date'] = pd.to_datetime(social_media_ground_truth['date'])\n",
    "    google_search_data_ground_truth['date'] = pd.to_datetime(google_search_data_ground_truth['date'])\n",
    "    \n",
    "    return sales_ground_truth, social_media_ground_truth, google_search_data_ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_ground_truth, social_media_ground_truth, google_search_data_ground_truth = convert_date_columns_to_datetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_merged_data(sales_ground_truth: pd.DataFrame, social_media_ground_truth: pd.DataFrame, google_search_ground_truth : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates the final ground truth to use further downstream.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    sales_ground_truth : pd.DataFrame\n",
    "        The sales ground truth to use for the final merging process.\n",
    "    social_media_ground_truth : pd.DataFrame\n",
    "        The social ground truth data to use for the final merging process.\n",
    "    google_search_ground_truth : pd.DataFrame\n",
    "        The google search ground truth data to use for the final merging process.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The merged dataframe\n",
    "    \"\"\"\n",
    "    combined_ground_truth = pd.merge(sales_ground_truth, social_media_ground_truth, on=['date', 'theme_name'])\n",
    "    combined_ground_truth = pd.merge(combined_ground_truth, google_search_ground_truth, on=['date', 'theme_name'])\n",
    "    return combined_ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ground_truth = create_final_merged_data(sales_ground_truth, social_media_ground_truth, google_search_data_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2940, 12)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_ground_truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_vendor_wise_data(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process the vendor wise data.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    dataframe : pd.DataFrame\n",
    "        The dataframe containing the vendor wise data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The processed dataframe containing client and competitor related features.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Client related features\n",
    "    client_dataframe = dataframe[dataframe['vendor'] == 'A']\n",
    "    client_dataframe.rename(columns={'weekly_lbs_value': 'client_lbs_value'\n",
    "                                     , 'weekly_units_value': 'client_units_value'\n",
    "                                     , 'weekly_dollars_value': 'client_dollars_value'}, inplace=True)\n",
    "    \n",
    "    # Competitor related features\n",
    "    competitor_dataframe = dataframe[dataframe['vendor'] != 'A']\n",
    "    competitor_dataframe['competitor_lbs_value'] = competitor_dataframe.groupby(['date', 'theme_name'])['weekly_lbs_value'].transform('sum')\n",
    "    competitor_dataframe['competitor_units_value'] = competitor_dataframe.groupby(['date', 'theme_name'])['weekly_units_value'].transform('sum')\n",
    "    competitor_dataframe['competitor_dollars_value'] = competitor_dataframe.groupby(['date', 'theme_name'])['weekly_dollars_value'].transform('sum')\n",
    "    \n",
    "    client_dataframe = client_dataframe[['date', 'theme_name', 'client_lbs_value', 'client_units_value', 'client_dollars_value']]\n",
    "    competitor_dataframe = competitor_dataframe[['date', 'theme_name', 'competitor_lbs_value', 'competitor_units_value', 'competitor_dollars_value']]\n",
    "    \n",
    "    dataframe = pd.merge(dataframe, client_dataframe, on=['date', 'theme_name'])\n",
    "    dataframe = pd.merge(dataframe, competitor_dataframe, on=['date', 'theme_name'])\n",
    "    \n",
    "    dataframe = dataframe[['date', 'theme_name', 'total_post'\n",
    "                                                    , 'google_searchVolume', 'amazon_searchVolume'\n",
    "                                                    , 'chewy_searchVolume', 'walmart_searchVolume'\n",
    "                                                    , 'total_searchVolume', 'client_lbs_value',\n",
    "                                                    'client_units_value', 'client_dollars_value',\n",
    "                                                    'competitor_lbs_value', 'competitor_units_value',\n",
    "                                                    'competitor_dollars_value'\n",
    "                                                   ]]\n",
    "    dataframe.drop_duplicates(inplace=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ground_truth = process_vendor_wise_data(combined_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>theme_name</th>\n",
       "      <th>total_post</th>\n",
       "      <th>google_searchVolume</th>\n",
       "      <th>amazon_searchVolume</th>\n",
       "      <th>chewy_searchVolume</th>\n",
       "      <th>walmart_searchVolume</th>\n",
       "      <th>total_searchVolume</th>\n",
       "      <th>client_lbs_value</th>\n",
       "      <th>client_units_value</th>\n",
       "      <th>client_dollars_value</th>\n",
       "      <th>competitor_lbs_value</th>\n",
       "      <th>competitor_units_value</th>\n",
       "      <th>competitor_dollars_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-09</td>\n",
       "      <td>salmon</td>\n",
       "      <td>47</td>\n",
       "      <td>1913.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1913.0</td>\n",
       "      <td>5.379338e+06</td>\n",
       "      <td>754859</td>\n",
       "      <td>7.639690e+06</td>\n",
       "      <td>4.554449e+06</td>\n",
       "      <td>657551</td>\n",
       "      <td>4.899568e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2016-02-06</td>\n",
       "      <td>salmon</td>\n",
       "      <td>47</td>\n",
       "      <td>850.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>850.0</td>\n",
       "      <td>5.217845e+06</td>\n",
       "      <td>745221</td>\n",
       "      <td>7.433237e+06</td>\n",
       "      <td>5.106687e+06</td>\n",
       "      <td>645078</td>\n",
       "      <td>4.990354e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2016-02-20</td>\n",
       "      <td>salmon</td>\n",
       "      <td>156</td>\n",
       "      <td>1063.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1063.0</td>\n",
       "      <td>4.747996e+06</td>\n",
       "      <td>687313</td>\n",
       "      <td>6.826838e+06</td>\n",
       "      <td>4.962688e+06</td>\n",
       "      <td>669369</td>\n",
       "      <td>4.958131e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2016-02-27</td>\n",
       "      <td>salmon</td>\n",
       "      <td>90</td>\n",
       "      <td>1912.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1912.0</td>\n",
       "      <td>4.676554e+06</td>\n",
       "      <td>680633</td>\n",
       "      <td>6.724496e+06</td>\n",
       "      <td>4.954023e+06</td>\n",
       "      <td>667029</td>\n",
       "      <td>4.956457e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>2016-03-26</td>\n",
       "      <td>salmon</td>\n",
       "      <td>50</td>\n",
       "      <td>1912.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1912.0</td>\n",
       "      <td>4.739258e+06</td>\n",
       "      <td>705062</td>\n",
       "      <td>6.837560e+06</td>\n",
       "      <td>4.807323e+06</td>\n",
       "      <td>640089</td>\n",
       "      <td>4.817914e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17430</th>\n",
       "      <td>2019-09-07</td>\n",
       "      <td>low carb</td>\n",
       "      <td>1891</td>\n",
       "      <td>38008.0</td>\n",
       "      <td>5236.0</td>\n",
       "      <td>4489.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>48360.0</td>\n",
       "      <td>4.028791e+06</td>\n",
       "      <td>2695393</td>\n",
       "      <td>1.331764e+07</td>\n",
       "      <td>2.992734e+07</td>\n",
       "      <td>12652417</td>\n",
       "      <td>8.959849e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17472</th>\n",
       "      <td>2019-09-14</td>\n",
       "      <td>low carb</td>\n",
       "      <td>1142</td>\n",
       "      <td>41306.0</td>\n",
       "      <td>4655.0</td>\n",
       "      <td>4862.0</td>\n",
       "      <td>669.0</td>\n",
       "      <td>51492.0</td>\n",
       "      <td>4.017835e+06</td>\n",
       "      <td>2682848</td>\n",
       "      <td>1.327477e+07</td>\n",
       "      <td>2.982720e+07</td>\n",
       "      <td>12612445</td>\n",
       "      <td>8.869762e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17514</th>\n",
       "      <td>2019-09-21</td>\n",
       "      <td>low carb</td>\n",
       "      <td>1158</td>\n",
       "      <td>35338.0</td>\n",
       "      <td>3699.0</td>\n",
       "      <td>2952.0</td>\n",
       "      <td>625.0</td>\n",
       "      <td>42614.0</td>\n",
       "      <td>3.974981e+06</td>\n",
       "      <td>2641925</td>\n",
       "      <td>1.313288e+07</td>\n",
       "      <td>2.985235e+07</td>\n",
       "      <td>12537084</td>\n",
       "      <td>8.910157e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17556</th>\n",
       "      <td>2019-09-28</td>\n",
       "      <td>low carb</td>\n",
       "      <td>1545</td>\n",
       "      <td>33610.0</td>\n",
       "      <td>6315.0</td>\n",
       "      <td>2661.0</td>\n",
       "      <td>960.0</td>\n",
       "      <td>43546.0</td>\n",
       "      <td>3.980566e+06</td>\n",
       "      <td>2684050</td>\n",
       "      <td>1.301893e+07</td>\n",
       "      <td>2.997781e+07</td>\n",
       "      <td>12565509</td>\n",
       "      <td>8.939163e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17598</th>\n",
       "      <td>2019-10-05</td>\n",
       "      <td>low carb</td>\n",
       "      <td>1932</td>\n",
       "      <td>16560.0</td>\n",
       "      <td>2569.0</td>\n",
       "      <td>1701.0</td>\n",
       "      <td>458.0</td>\n",
       "      <td>21288.0</td>\n",
       "      <td>4.094692e+06</td>\n",
       "      <td>2725010</td>\n",
       "      <td>1.341184e+07</td>\n",
       "      <td>3.062778e+07</td>\n",
       "      <td>12827470</td>\n",
       "      <td>9.129245e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>420 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date theme_name  total_post  google_searchVolume  \\\n",
       "0     2016-01-09     salmon          47               1913.0   \n",
       "42    2016-02-06     salmon          47                850.0   \n",
       "84    2016-02-20     salmon         156               1063.0   \n",
       "126   2016-02-27     salmon          90               1912.0   \n",
       "168   2016-03-26     salmon          50               1912.0   \n",
       "...          ...        ...         ...                  ...   \n",
       "17430 2019-09-07   low carb        1891              38008.0   \n",
       "17472 2019-09-14   low carb        1142              41306.0   \n",
       "17514 2019-09-21   low carb        1158              35338.0   \n",
       "17556 2019-09-28   low carb        1545              33610.0   \n",
       "17598 2019-10-05   low carb        1932              16560.0   \n",
       "\n",
       "       amazon_searchVolume  chewy_searchVolume  walmart_searchVolume  \\\n",
       "0                      0.0                 0.0                   0.0   \n",
       "42                     0.0                 0.0                   0.0   \n",
       "84                     0.0                 0.0                   0.0   \n",
       "126                    0.0                 0.0                   0.0   \n",
       "168                    0.0                 0.0                   0.0   \n",
       "...                    ...                 ...                   ...   \n",
       "17430               5236.0              4489.0                 627.0   \n",
       "17472               4655.0              4862.0                 669.0   \n",
       "17514               3699.0              2952.0                 625.0   \n",
       "17556               6315.0              2661.0                 960.0   \n",
       "17598               2569.0              1701.0                 458.0   \n",
       "\n",
       "       total_searchVolume  client_lbs_value  client_units_value  \\\n",
       "0                  1913.0      5.379338e+06              754859   \n",
       "42                  850.0      5.217845e+06              745221   \n",
       "84                 1063.0      4.747996e+06              687313   \n",
       "126                1912.0      4.676554e+06              680633   \n",
       "168                1912.0      4.739258e+06              705062   \n",
       "...                   ...               ...                 ...   \n",
       "17430             48360.0      4.028791e+06             2695393   \n",
       "17472             51492.0      4.017835e+06             2682848   \n",
       "17514             42614.0      3.974981e+06             2641925   \n",
       "17556             43546.0      3.980566e+06             2684050   \n",
       "17598             21288.0      4.094692e+06             2725010   \n",
       "\n",
       "       client_dollars_value  competitor_lbs_value  competitor_units_value  \\\n",
       "0              7.639690e+06          4.554449e+06                  657551   \n",
       "42             7.433237e+06          5.106687e+06                  645078   \n",
       "84             6.826838e+06          4.962688e+06                  669369   \n",
       "126            6.724496e+06          4.954023e+06                  667029   \n",
       "168            6.837560e+06          4.807323e+06                  640089   \n",
       "...                     ...                   ...                     ...   \n",
       "17430          1.331764e+07          2.992734e+07                12652417   \n",
       "17472          1.327477e+07          2.982720e+07                12612445   \n",
       "17514          1.313288e+07          2.985235e+07                12537084   \n",
       "17556          1.301893e+07          2.997781e+07                12565509   \n",
       "17598          1.341184e+07          3.062778e+07                12827470   \n",
       "\n",
       "       competitor_dollars_value  \n",
       "0                  4.899568e+06  \n",
       "42                 4.990354e+06  \n",
       "84                 4.958131e+06  \n",
       "126                4.956457e+06  \n",
       "168                4.817914e+06  \n",
       "...                         ...  \n",
       "17430              8.959849e+07  \n",
       "17472              8.869762e+07  \n",
       "17514              8.910157e+07  \n",
       "17556              8.939163e+07  \n",
       "17598              9.129245e+07  \n",
       "\n",
       "[420 rows x 14 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_for_modelling = combined_ground_truth[['date', 'theme_name', 'total_post'\n",
    "                                                    , 'google_searchVolume', 'amazon_searchVolume'\n",
    "                                                    , 'chewy_searchVolume', 'walmart_searchVolume'\n",
    "                                                    , 'total_searchVolume', 'client_lbs_value',\n",
    "                                                    'client_units_value', 'client_dollars_value',\n",
    "                                                    'competitor_lbs_value', 'competitor_units_value',\n",
    "                                                    'competitor_dollars_value'\n",
    "                                                   ]]\n",
    "ground_truth_for_modelling.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset(context, ground_truth_for_modelling, '/processed/ground_truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X = ground_truth_for_modelling.drop(columns=['client_lbs_value'])\n",
    "target = y = ground_truth_for_modelling['client_lbs_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/raw/orders',\n",
       " '/raw/product',\n",
       " '/cleaned/orders',\n",
       " '/cleaned/product',\n",
       " '/cleaned/sales',\n",
       " '/processed/google_search',\n",
       " '/processed/sales',\n",
       " '/processed/social_media',\n",
       " '/processed/ground_truth',\n",
       " '/train/sales/features',\n",
       " '/train/sales/target',\n",
       " '/test/sales/features',\n",
       " '/test/sales/target',\n",
       " '/score/sales/output']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_datasets(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Move this to 2nd notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset(context, X_train, '/train/sales/features')\n",
    "save_dataset(context, y_train, '/train/sales/target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset(context, X_test, '/test/sales/features')\n",
    "save_dataset(context, y_test, '/test/sales/target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Feature Engineering\n",
    "\n",
    "The focus here is the `Pipeline` and not the model. Though the model would inform the pipeline that is needed to train the model, our focus is to set it up in such a way that it can be saved/loaded, tweaked for different model choices and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Read the Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check why the shape is having a mismatch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(294, 13) (294, 1)\n",
      "(126, 13) (126, 1)\n"
     ]
    }
   ],
   "source": [
    "train_X = load_dataset(context, 'train/sales/features')\n",
    "train_y = load_dataset(context, 'train/sales/target')\n",
    "print(train_X.shape, train_y.shape)\n",
    "\n",
    "test_X = load_dataset(context, 'test/sales/features')\n",
    "test_y = load_dataset(context, 'test/sales/target')\n",
    "print(test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Feature Engineering Pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dev NOTES**\n",
    "\n",
    "For Feature Engineering and Model Building sklearn.pipeline.Pipeline are leveraged because of the following advantages\n",
    "<details>\n",
    "    \n",
    "1. It helps in automating workflows and are easier to read and comprehend.\n",
    "2. Right Sequence can be ensured and (for example always encodes before imputing)\n",
    "3. Reproducibility is very convenient with pipelines\n",
    "4. Pipelines help you prevent data leakage in your test data\n",
    "5. Code is near implementation ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Steps in the Feature Transformation are as follows\n",
    " - Outlier Treatment\n",
    " - Encoding of Categorical Columns\n",
    " - Missing Values Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting different types of columns for transformations\n",
    "cat_columns = train_X.select_dtypes('object').columns\n",
    "num_columns = train_X.select_dtypes('number').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['theme_name'], dtype='object')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['total_post', 'google_searchVolume', 'amazon_searchVolume',\n",
       "       'chewy_searchVolume', 'walmart_searchVolume', 'total_searchVolume',\n",
       "       'client_units_value', 'client_dollars_value', 'competitor_lbs_value',\n",
       "       'competitor_units_value', 'competitor_dollars_value'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier Handling\n",
    "- A Custom Transformer is used to handle outliers. It is not included as part of the pipeline as outliers handling are optional for test data\n",
    "- An option to either drop or cap the outliers can be passed during the transform call\n",
    "- If we want to treat outliers for some columns them we can pass cols argument to the Transformer\n",
    "- This will go into production code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(294, 13)\n",
      "(294, 13)\n"
     ]
    }
   ],
   "source": [
    "outlier_transformer = Outlier(method='median')\n",
    "print(train_X.shape)\n",
    "train_X = outlier_transformer.fit_transform(train_X)\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some sample pipelines showcasing how to create column specific pipelines and integrating them overall is presented below\n",
    "\n",
    "- Commonly target encoding is done for categorical variables with too many levels.\n",
    "- We also group sparse levels. For fewer levels one hot encoding/label encoding is preferred.\n",
    "- If there is one dominant level, we can use binary encoding.\n",
    "- This will go into production code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Remove mode imputer\n",
    "cat_imputer = Pipeline([\n",
    "    ('simple_impute', SimpleImputer(strategy='most_frequent')),\n",
    "])\n",
    "\n",
    "\n",
    "# NOTE: the list of transformations here are not sequential but weighted \n",
    "# (if multiple transforms are specified for a particular column)\n",
    "# for sequential transforms use a pipeline as shown above.\n",
    "features_transformer = ColumnTransformer([\n",
    "    \n",
    "    ## categorical columns\n",
    "    ('one_hot_enc', OneHotEncoder(drop='first'),\n",
    "     list(set(cat_columns))),\n",
    "    \n",
    "    # NOTE: if the same column gets repeated, then they are weighed in the final output\n",
    "    # If we want a sequence of operations, then we use a pipeline but that doesen't YET support\n",
    "    # get_feature_names. \n",
    "    ('cat_variable_imputer', cat_imputer, ['theme_name']),\n",
    "        \n",
    "    ## numeric columns\n",
    "    ('med_enc', SimpleImputer(strategy='median'), num_columns),\n",
    "    \n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dev notes(Encoding):**\n",
    "<details>\n",
    "\n",
    "    Some common practices followed in Categorical Feature Encoding are\n",
    "    * For categorical variables with too many levels, target encoding can be done.\n",
    "    * For fewer levels, one hot encoding can be done.\n",
    "    * If one very dominant level is observed, binary encoding can be used.\n",
    "    \n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Feature analysis\n",
    "\n",
    "Using the pipeline above analyze the features and decide on additional features to add/remove from the pipeline. This section will not be part of the production code, unless input data drifts etc. are explicitly demanded in the project.\n",
    "\n",
    "Here we are primarily focused on feature selection/elimination based on business rules, prior knowledge, data analysis.\n",
    "\n",
    "**We are not building any models at this point.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we create some sample data to analyze that we assume represent the population\n",
    "- train the features transformer and do the analysis as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_X = train_X.sample(frac=0.1, random_state=context.random_seed)\n",
    "sample_y = train_y.loc[sample_X.index]\n",
    "\n",
    "sample_train_X = get_dataframe(\n",
    "    features_transformer.fit_transform(sample_X, sample_y), \n",
    "    get_feature_names_from_column_transformer(features_transformer)\n",
    ")\n",
    "\n",
    "# nothing to do for target\n",
    "sample_train_y = sample_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the features transformer on the complete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = get_dataframe(\n",
    "    features_transformer.fit_transform(train_X, train_y), \n",
    "    get_feature_names_from_column_transformer(features_transformer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check why the additional column of cat_variable_imputer_theme_name is being created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Univariate\n",
    "\n",
    "\n",
    "- Look at each variable independently. This is useful if your models have assumptions on the distribution and/or bounds on the features/target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove this , temporary code\n",
    "train_X = train_X.drop(columns=['cat_variable_imputer_theme_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X['theme_name_no additives/preservatives'] = train_X['theme_name_no additives/preservatives'].astype(int)\n",
    "train_X['theme_name_salmon'] = train_X['theme_name_salmon'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in num_columns:\n",
    "    train_X[column] = train_X[column].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = eda.get_density_plots(train_X, cols=train_X.columns.tolist())\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the plots are html\n",
    "reports.create_report({'univariate': out}, name='feature_analysis_univariate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports.feature_analysis(train_X,'./feature_analysis_report.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Bivariate - mutual interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find columns with high correlations and drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = eda.get_correlation_table(train_X)\n",
    "out[out[\"Abs Corr Coef\"] > 0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping total search volume as its highly correlated with google search volume\n",
    "# Dropping chewy search volume as its highly correlated with amazon search volume\n",
    "# We will be going forward with the rest of the features as they represent key \n",
    "# features to account for after the modelling stage(despite the high correlations).\n",
    "curated_columns = list(\n",
    "    set(train_X.columns.to_list()) \n",
    "    - set(['total_searchVolume', 'chewy_searchVolume'])\n",
    ")\n",
    "\n",
    "train_X = train_X[curated_columns]\n",
    "\n",
    "out = eda.get_correlation_table(train_X)\n",
    "out[out[\"Abs Corr Coef\"] > 0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = eda.get_bivariate_plots(train_X, x_cols=['theme_name_salmon'], y_cols=['theme_name_no additives/preservatives'])\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create reports as needed\n",
    "cols = train_X.columns.to_list()\n",
    "all_plots = {}\n",
    "for ii, col1 in enumerate(cols): \n",
    "    for jj in range(ii+1, len(cols)):\n",
    "        col2 = cols[jj]\n",
    "        out = eda.get_bivariate_plots(train_X, x_cols=[col1], y_cols=[col2])\n",
    "        all_plots.update({f'{col2} vs {col1}': out})\n",
    "\n",
    "reports.create_report(all_plots, name='feature_analysis_bivariate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports.feature_interactions(train_X,'./feature_interaction_report.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Key Drivers - Interaction with Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = eda.get_target_correlation(train_X, train_y, y_continuous=True)\n",
    "display_as_tabs([(k, v) for k,v in out.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y['client_lbs_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = eda.get_feature_importances(train_X, train_y, y_continuous=True)\n",
    "display_as_tabs([(k, v) for k,v in out.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key drivers report like feature importance, bivariate plots can be obtained as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports.key_drivers(train_X,train_y, './key_drivers_report.html', y_continuous=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dev Notes**\n",
    "<details>\n",
    "    \n",
    "- The SHAP plots and bivariate plots in key drivers reports can be obtained by including quick=False as a parameter to key_drivers function call. \n",
    "- SHAP plots and bivariate plots often take long depending on data shape.\n",
    "- The plot with shap is present [here](https://drive.google.com/file/d/1JOTMBLiv3LEqZ-kxZz0RokW9v5UyiGva/view?usp=sharing)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "All the plots like feature analysis, interaction, key drivers can be obtained as a single plot using data exploration method as shown below. The output from this is available [here](https://drive.google.com/file/d/1209MzmSSEhiTYuPfHpaVXFXUVbkaJm0B/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports.data_exploration(train_X,train_y,'./data_exploration_report.html', y_continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the list of relevant columns\n",
    "save_pipeline(curated_columns, op.abspath(op.join(artifacts_folder, 'curated_columns.joblib')))\n",
    "\n",
    "# save the feature pipeline\n",
    "save_pipeline(features_transformer, op.abspath(op.join(artifacts_folder, 'features.joblib')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Modelling - Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Feature Selection(Specific to Regression)\n",
    "\n",
    "- Selecting Features specific to regression\n",
    "- VIF : measure of the amount of multi-collinearity in a set of multiple regressor variables. \n",
    "- On a case to case basis VIF thresholds change. Generally 5 or 10 are acceptable levels.\n",
    "- Usually on a recursive basis when removing the most collinear variable, there can be shuffle in VIF. \n",
    "- Often this section will not be part of the production code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make sure client and competitor features are not getting removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(train_X.columns)\n",
    "vif = eda.calc_vif(train_X)\n",
    "while max(vif.VIF) > 12:\n",
    "    #removing the largest variable from VIF\n",
    "    cols.remove(vif[(vif.VIF==vif.VIF.max())].variables.tolist()[0])\n",
    "    vif = eda.calc_vif(train_X[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_vars = vif.query('VIF < 11').variables\n",
    "reg_vars = list(reg_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transformations like these can be utilised\n",
    "def _custom_data_transform(df, cols2keep=None):\n",
    "    \"\"\"Transformation to drop some columns in the data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        df - pd.DataFrame\n",
    "        cols2keep - columns to keep in the dataframe\n",
    "    \"\"\"\n",
    "    cols2keep = cols2keep or []\n",
    "    if len(cols2keep):\n",
    "        return (df\n",
    "                .select_columns(cols2keep))\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Model training pipeline\n",
    "\n",
    "- Here we focus on creating a collection of pipelines that can be used for training respective models.\n",
    "- Each model pipeline will essentially be of the form\n",
    "```\n",
    "[\n",
    "('preprocessing', preprocessing_pipeline),\n",
    "('feature_selection', feature_selection_pipeline),\n",
    "('estimator', estimator),\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Add feature engineering section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 Model Pipeline Build\n",
    "\n",
    "- This will be part of the production code (training only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ppln_ols = Pipeline([\n",
    "    ('',FunctionTransformer(_custom_data_transform, kw_args={'cols2keep':reg_vars})),\n",
    "    ('estimator', SKLStatsmodelOLS())\n",
    "])\n",
    "reg_ppln_ols.fit(train_X, train_y.values.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ppln_ols['estimator'].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.5 Model Evaluation(Linear Model)\n",
    "\n",
    "This will be part of the production code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ppln = Pipeline([\n",
    "    ('', FunctionTransformer(_custom_data_transform, kw_args={'cols2keep':reg_vars})),\n",
    "    ('Linear Regression', SKLStatsmodelOLS())\n",
    "])\n",
    "\n",
    "test_X = get_dataframe(\n",
    "    features_transformer.transform(test_X), \n",
    "    get_feature_names_from_column_transformer(features_transformer)\n",
    ")\n",
    "test_X = test_X[curated_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_linear_report = RegressionReport(model=reg_ppln, x_train=train_X, y_train=train_y, x_test= test_X, y_test= test_y, refit=True)\n",
    "reg_linear_report.get_report(include_shap=False, file_path='regression_linear_model_report')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dev Notes**\n",
    "Use SHAP for variable interpretability.\n",
    "<details>\n",
    "\n",
    "    1. Use SHAP=True to generate variable interpretability plots in the report\n",
    "    2. SHAP is recommended for non parameteric models such as RF, xgboost.\n",
    "    3. However, SHAP reports are time consuming depending on no.of records and model complexity.\n",
    "    \n",
    "A sample of regerssion report with SHAP can be found [here](https://drive.google.com/file/d/18RlQTsT1ze09Cgz-qpb4ha_cvyWbN5F5/view?usp=sharing).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.6 Residual Analysis\n",
    "- After scoring the model, it is recommended to do a residual analysis to know the distribution of errors\n",
    "- we took a threshold of 30% above which it is marked as over prediction or underprediction\n",
    "- This will not be part of the production code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.3\n",
    "residual_analysis = test_X.copy()\n",
    "residual_analysis['prediction'] = reg_ppln_ols.predict(test_X)\n",
    "residual_analysis['actuals'] = test_y.reset_index(drop = True).iloc[:,0].values\n",
    "residual_analysis['forecast_flag'] = 'good'\n",
    "residual_analysis.loc[((residual_analysis['prediction'] > (1+threshold) * residual_analysis['actuals'])\\\n",
    "                       & (residual_analysis['actuals']>100)),'forecast_flag'] = 'over predict'\n",
    "residual_analysis.loc[((residual_analysis['prediction'] < (1-threshold) * residual_analysis['actuals'])\\\n",
    "                       & (residual_analysis['actuals']>100)),'forecast_flag'] = 'under predict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_analysis.hvplot.kde(y=\"client_lbs_value\",by=\"forecast_flag\", ## Grouping by Predictions\n",
    "                                width=800, height=400,\n",
    "                                alpha=0.7,\n",
    "                                ylabel=\"density\",\n",
    "                                xlabel=\"unit_cost\",\n",
    "                                title=f'unit cost(density)',legend='top_right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the above plot we can infer that the higher \"over predictions\" are happening for unit_cost > 200.\n",
    "- similarly, the higher \"under predictions\" are happening for unit_cost is zero.\n",
    "\n",
    "This can help us tune the model by a separate model for unit_cost > 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Modelling - XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.1 Model training pipeline\n",
    "\n",
    "Here we focus on creating a collection of pipelines that can be used for tranining respective models.\n",
    "\n",
    "Each model pipeline will essentially be of the form\n",
    "```\n",
    "[\n",
    "('preprocessing', preprocessing_pipeline),\n",
    "('feature_selection', feature_selection_pipeline),\n",
    "('estimator', estimator),\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Model Pipeline Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's find features for some decent defaults\n",
    "estimator = XGBRegressor()\n",
    "xgb_training_pipe_init = Pipeline([\n",
    "    ('XGBoost', XGBRegressor())\n",
    "])\n",
    "xgb_training_pipe_init.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding the Feature Importance\n",
    "%matplotlib inline\n",
    "imp = pd.DataFrame({'importance': xgb_training_pipe_init['XGBoost'].feature_importances_})\n",
    "imp.index = train_X.columns\n",
    "imp.sort_values('importance',inplace=True)\n",
    "imp.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'condition','model_family','days_since_last_purchase','first_time_customer','sales_person', are considered to be important and in grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pipeline build based on new importance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's find features for some decent defaults\n",
    "imp_features = ['model_family','sku','unit_cost','condition','brand','business_unit']\n",
    "\n",
    "estimator = XGBRegressor()\n",
    "xgb_training_pipe2 = Pipeline([\n",
    "    ('', FunctionTransformer(_custom_data_transform, kw_args={'cols2keep':imp_features})),\n",
    "    ('XGBoost', XGBRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search of the Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "parameters = {\n",
    "   'gamma':[0.03],\n",
    "   'min_child_weight':[6],\n",
    "   'learning_rate':[0.1],\n",
    "   'max_depth':[3],\n",
    "   'n_estimators':[500], \n",
    "}\n",
    "est = XGBRegressor()\n",
    "xgb_grid = GridSearchCV(est,\n",
    "                        parameters,\n",
    "                        cv = 2,\n",
    "                        n_jobs = 4,\n",
    "                        verbose=True)\n",
    "\n",
    "xgb_grid.fit(train_X, train_y)\n",
    "\n",
    "print(xgb_grid.best_score_)\n",
    "print(xgb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Build using the best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pipeline_final = Pipeline([\n",
    "    ('', FunctionTransformer(_custom_data_transform, kw_args={'cols2keep':imp_features})),\n",
    "    ('XGBoost', xgb_grid.best_estimator_)\n",
    "])\n",
    "xgb_pipeline_final.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_tree_report = RegressionReport(model=xgb_pipeline_final, x_train=train_X, y_train=train_y, x_test= test_X, y_test= test_y)\n",
    "reg_tree_report.get_report(include_shap=False, file_path='regression_tree_model_report')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Regression report containing the feature importances are available [here](https://drive.google.com/file/d/1JBfL3uxPcxBfl0amweXBFmLr7CSHFBUO/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a comparison report of the  linear (vs) tree -based model  approach can be generated as follows.\n",
    "\n",
    "This code will not be part of the production code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipelines = [reg_ppln, xgb_pipeline_final]\n",
    "model_comparison_report = RegressionComparison(models=model_pipelines,x=train_X, y=train_y)\n",
    "metrics = model_comparison_report.get_report(file_path='regression_comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison_report.performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A report comparing the performance, metrics between Linear model and Tree model are available [here](https://drive.google.com/file/d/1LDibiFap9K4DKME-Y0S0mtI_05lTdaJF/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dev NOTES**\n",
    "<details>\n",
    "\n",
    "the above metrics are absolute nos and not %ges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we are choosing LM model for pipelining. General criteria for choosing production models is:\n",
    "\n",
    "- Parametric models (aka whitebox models) such as Linear Regression are easier to explain to non-technical audience.\n",
    "- Generally these are accepted fast and adoption is quicker.\n",
    "- If the downstream calls for optimization using these models parametric models are easier to implement.\n",
    "- When accuracy is primary goal without explainability, the above two takes a backseat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
